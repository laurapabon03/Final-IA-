{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurapabon03/Final-IA-/blob/main/Copy_of_Final_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parcial Final Inteligencia Artificial\n",
        "\n",
        "√Ångela Sofia Torres, Juan David Salda√±a Rivera, Laura Pab√≥n, Dario Montoya, Lucas Alvarado"
      ],
      "metadata": {
        "id": "26NxRaHnuLYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementaci√≥n de RAG (Retrieval Augmented Generation) con Ollama para revisi√≥n de documentos econ√≥micos. Este script implementa un sistema de Generaci√≥n Aumentada por Recuperaci√≥n que a√±ade informaci√≥n sobre documentos econ√≥micos para consultas sobre documentos usando Ollama para inferencia r√°pida.\n",
        "\n"
      ],
      "metadata": {
        "id": "IDd244CXGsc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 1: INSTALACI√ìN DE DEPENDENCIAS\n",
        "# Estas l√≠neas instalan las bibliotecas necesarias para el funcionamiento del sistema\n",
        "# Se ejecutan solamente en entornos como Google Colab o Jupyter Notebooks\n",
        "\n",
        "# Instalaci√≥n de bibliotecas esenciales para RAG\n",
        "!pip install -q langchain langchain_community sentence-transformers pypdf python-docx docx2txt unstructured faiss-cpu gradio\n",
        "!pip install -q chromadb requests\n",
        "!pip install -q pandas\n",
        "!pip install -q pymupdf requests"
      ],
      "metadata": {
        "id": "IX_VoOPvKg9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 2: CONFIGURACI√ìN DE OLLAMA\n",
        "# Ollama es una herramienta que permite ejecutar modelos de lenguaje localmente con menor uso de recursos que otros sistemas\n",
        "\n",
        "# Instalaci√≥n de Ollama\n",
        "print(\"Instalando Ollama para inferencia r√°pida...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Iniciar el servicio de Ollama en segundo plano\n",
        "print(\"\\nIniciando servidor Ollama...\")\n",
        "!pkill ollama || true  # Detener cualquier instancia anterior\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar en segundo plano\n",
        "\n",
        "# Dar tiempo al servidor para inicializar\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama est√© listo...\")\n",
        "time.sleep(15)  # Esperar 15 segundos\n",
        "\n",
        "# Verificar que el servidor est√© activo\n",
        "print(\"\\nVerificando que el servidor Ollama est√© respondiendo...\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no est√° respondiendo\"\n",
        "\n",
        "# Descargar el modelo LLM que usaremos (Llama2)\n",
        "print(\"\\nDescargando modelo llama2 desde Ollama...\")\n",
        "!ollama pull llama2"
      ],
      "metadata": {
        "id": "nyJWf9CNKpA4",
        "outputId": "e3c5a582-123b-4b36-a4a4-2ced2501f271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Ollama para inferencia r√°pida...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "Iniciando servidor Ollama...\n",
            "Esperando a que el servidor Ollama est√© listo...\n",
            "\n",
            "Verificando que el servidor Ollama est√© respondiendo...\n",
            "{\"models\":[{\"name\":\"llama2:latest\",\"model\":\"llama2:latest\",\"modified_at\":\"2025-05-16T02:51:17.825653216Z\",\"size\":3826793677,\"digest\":\"78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"7B\",\"quantization_level\":\"Q4_0\"}}]}\n",
            "Descargando modelo llama2 desde Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SECCI√ìN 3: IMPORTACI√ìN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "from langchain_community.llms import Ollama  # Integraci√≥n LangChain-Ollama\n"
      ],
      "metadata": {
        "id": "lTOu6XV4MqlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 4: CONFIGURACI√ìN B√ÅSICA\n",
        "\n",
        "# Configuraci√≥n de logs para monitoreo y depuraci√≥n\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constantes para configuraci√≥n del sistema\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".csv\", \".txt\"]  # Formatos soportados\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"  # Modelo para codificaci√≥n sem√°ntica\n",
        "OLLAMA_MODEL = \"llama2\"  # Modelo LLM local\n"
      ],
      "metadata": {
        "id": "NgJrarcKOpDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 5: CLASE PARA CARGA DE DOCUMENTOS\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Cargador unificado de documentos que soporta m√∫ltiples formatos.\n",
        "    Esta clase selecciona el cargador adecuado seg√∫n la extensi√≥n del archivo.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> List:\n",
        "        \"\"\"\n",
        "        Carga un archivo basado en su extensi√≥n y devuelve los documentos procesados.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo a cargar\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos procesados con sus metadatos\n",
        "        \"\"\"\n",
        "        print(f\"Cargando archivo: {file_path}\")\n",
        "        ext = os.path.splitext(file_path)[1].lower()  # Obtener extensi√≥n del archivo\n",
        "\n",
        "        try:\n",
        "            # Seleccionar el cargador apropiado seg√∫n el tipo de archivo\n",
        "            if ext == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)  # Para archivos PDF\n",
        "            elif ext in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)  # Para documentos Word\n",
        "            elif ext == '.csv':\n",
        "                loader = CSVLoader(file_path)  # Para archivos CSV\n",
        "            else:  # Para txt y otros formatos de texto\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "            # Ejecutar la carga del documento\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Enriquecer con metadatos para mejorar la recuperaci√≥n y visualizaci√≥n\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    'title': os.path.basename(file_path),  # Nombre del archivo\n",
        "                    'type': 'document',  # Tipo de contenido\n",
        "                    'format': ext[1:],  # Formato sin el punto inicial\n",
        "                    'language': 'auto'  # Idioma (auto-detectado)\n",
        "                })\n",
        "\n",
        "            print(f\"‚úÖ Archivo cargado exitosamente: {file_path}\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al cargar {file_path}: {str(e)}\")\n",
        "            raise  # Re-lanzar la excepci√≥n para manejo superior"
      ],
      "metadata": {
        "id": "S0IV8BtROq-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 6: CLASE PRINCIPAL DEL SISTEMA RAG\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Sistema RAG completo con Ollama para consulta de documentos.\n",
        "\n",
        "    Esta clase implementa todo el flujo de trabajo RAG:\n",
        "    1. Carga y procesamiento de documentos\n",
        "    2. Generaci√≥n de embeddings y almacenamiento vectorial\n",
        "    3. Recuperaci√≥n de contexto relevante\n",
        "    4. Generaci√≥n de respuestas mediante LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL, ollama_model: str = OLLAMA_MODEL):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema RAG con los modelos especificados.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Modelo para generar embeddings (representaciones vectoriales)\n",
        "            ollama_model: Modelo de lenguaje a utilizar con Ollama\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.ollama_model = ollama_model\n",
        "        self.embeddings = None  # Se inicializar√° posteriormente\n",
        "        self.vector_store = None  # Base de datos vectorial\n",
        "        self.qa_chain = None  # Cadena de pregunta-respuesta\n",
        "        self.is_initialized = False  # Flag de inicializaci√≥n\n",
        "        self.processed_files = set()  # Conjunto para evitar procesar archivos duplicados\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"\n",
        "        Inicializa los componentes del sistema RAG:\n",
        "        - Modelo de embeddings\n",
        "        - Conexi√≥n con Ollama\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üöÄ Inicializando sistema RAG con Ollama...\")\n",
        "\n",
        "            # Inicializar el modelo de embeddings (usando CPU o GPU si est√° disponible)\n",
        "            print(\"üìä Cargando modelo de embeddings...\")\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=self.embedding_model,\n",
        "                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "                encode_kwargs={'normalize_embeddings': True}  # Normalizaci√≥n para mejor b√∫squeda\n",
        "            )\n",
        "\n",
        "            # Verificaci√≥n de salud de Ollama - reintento si no responde\n",
        "            try:\n",
        "                response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "                if response.status_code != 200:\n",
        "                    print(\"‚ö†Ô∏è Advertencia: Ollama no est√° respondiendo correctamente. Reintentando inicializaci√≥n...\")\n",
        "                    time.sleep(5)\n",
        "                    # Reinicio de emergencia del servicio Ollama\n",
        "                    subprocess.run(\"pkill ollama || true\", shell=True)\n",
        "                    subprocess.run(\"nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &\", shell=True)\n",
        "                    time.sleep(15)  # Esperar a que reinicie\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Advertencia al verificar Ollama: {str(e)}\")\n",
        "\n",
        "            # Configurar Ollama como modelo de lenguaje mediante LangChain\n",
        "            print(\"üß† Configurando Ollama como LLM...\")\n",
        "            self.llm = Ollama(\n",
        "                model=self.ollama_model,\n",
        "                temperature=0.1,  # Temperatura baja para respuestas m√°s deterministas\n",
        "                num_predict=512  # M√°ximo de tokens a generar\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True  # Marcar como inicializado\n",
        "            print(\"‚úÖ Sistema RAG inicializado correctamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error durante la inicializaci√≥n: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_documents(self, files: List[tempfile._TemporaryFileWrapper]) -> None:\n",
        "        \"\"\"\n",
        "        Procesa documentos cargados y actualiza la base de datos vectorial.\n",
        "\n",
        "        Args:\n",
        "            files: Lista de archivos temporales cargados por el usuario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            documents = []  # Lista para almacenar todos los documentos\n",
        "            new_files = []  # Seguimiento de archivos nuevos procesados\n",
        "\n",
        "            print(f\"üìÑ Procesando {len(files)} documento(s)...\")\n",
        "\n",
        "            # Filtrar y procesar solo archivos que no se han procesado antes\n",
        "            for file in files:\n",
        "                if file.name not in self.processed_files:\n",
        "                    docs = DocumentLoader.load_file(file.name)  # Cargar el archivo\n",
        "                    documents.extend(docs)  # A√±adir documentos a la lista\n",
        "                    new_files.append(file.name)  # Registrar como nuevo\n",
        "                    self.processed_files.add(file.name)  # Marcar como procesado\n",
        "\n",
        "            # Si no hay archivos nuevos, terminar\n",
        "            if not new_files:\n",
        "                print(\"‚ÑπÔ∏è No hay documentos nuevos para procesar\")\n",
        "                return\n",
        "\n",
        "            # Verificar que se hayan cargado documentos\n",
        "            if not documents:\n",
        "                raise ValueError(\"No se pudieron cargar documentos.\")\n",
        "\n",
        "            # --------- DIVISI√ìN DE DOCUMENTOS ---------\n",
        "            # Dividir documentos en fragmentos m√°s peque√±os para procesamiento eficiente\n",
        "            print(\"‚úÇÔ∏è Dividiendo documentos en fragmentos...\")\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=800,  # Tama√±o objetivo de cada fragmento (en caracteres)\n",
        "                chunk_overlap=200,  # Superposici√≥n entre fragmentos para mantener contexto\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separaci√≥n\n",
        "                length_function=len  # Funci√≥n para medir longitud\n",
        "            )\n",
        "\n",
        "            # Aplicar la divisi√≥n a todos los documentos\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            print(f\"üß© Documentos divididos en {len(chunks)} fragmentos\")\n",
        "\n",
        "            # --------- VECTORIZACI√ìN Y ALMACENAMIENTO ---------\n",
        "            # Crear o actualizar la base de datos vectorial con los nuevos fragmentos\n",
        "            print(\"üîç Vectorizando fragmentos...\")\n",
        "            if self.vector_store is None:\n",
        "                # Primera carga: crear nueva base de datos vectorial\n",
        "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "            else:\n",
        "                # Carga adicional: a√±adir a la base de datos existente\n",
        "                self.vector_store.add_documents(chunks)\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE PROMPT ---------\n",
        "            # Definir la plantilla de prompt para el LLM\n",
        "            prompt_template = \"\"\"\n",
        "            Contexto: {context}\n",
        "\n",
        "            Asume que eres un experto en econom√≠a. Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa, con un lenguaje y tono acad√©mico y formal\n",
        "            Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "            Pregunta: {question}\n",
        "            \"\"\"\n",
        "\n",
        "            # Crear objeto de prompt con variables\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template,\n",
        "                input_variables=[\"context\", \"question\"]  # Variables a rellenar\n",
        "            )\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE CADENA QA ---------\n",
        "            # Inicializar la cadena de pregunta-respuesta con Ollama\n",
        "            print(\"‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\")\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,  # Modelo de lenguaje\n",
        "                chain_type=\"stuff\",  # Tipo de cadena (insertar todo el contexto de una vez)\n",
        "                retriever=self.vector_store.as_retriever(\n",
        "                    search_kwargs={\"k\": 6}  # Recuperar los 6 fragmentos m√°s relevantes\n",
        "                ),\n",
        "                return_source_documents=True,  # Devolver documentos fuente para citas\n",
        "                chain_type_kwargs={\"prompt\": PROMPT}  # Usar nuestro prompt personalizado\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Procesamiento completado: {len(documents)} documentos a√±adidos a la base de conocimiento\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error procesando documentos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 1: GENERACI√ìN MEDIANTE LANGCHAIN (m√°s robusto)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_response(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Genera una respuesta utilizando el framework LangChain.\n",
        "        Este m√©todo es m√°s robusto y estructurado, con mejor manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la respuesta y fuentes utilizadas\n",
        "        \"\"\"\n",
        "        # Verificar que el sistema est√© inicializado\n",
        "        if not self.is_initialized or self.vector_store is None:\n",
        "            return {\n",
        "                'answer': \"Por favor, carga algunos documentos antes de hacer preguntas.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"‚ùì Procesando pregunta: {question}\")\n",
        "\n",
        "            # Ejecutar la cadena QA con LangChain y Ollama\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Preparar la respuesta estructurada\n",
        "            response = {\n",
        "                'answer': result['result'],  # Respuesta generada\n",
        "                'sources': []  # Lista para fuentes\n",
        "            }\n",
        "\n",
        "            # A√±adir informaci√≥n sobre las fuentes utilizadas\n",
        "            for doc in result['source_documents']:\n",
        "                source = {\n",
        "                    'title': doc.metadata.get('title', 'Desconocido'),\n",
        "                    'content': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    'metadata': doc.metadata\n",
        "                }\n",
        "                response['sources'].append(source)\n",
        "\n",
        "            print(\"‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\")\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generando respuesta con LangChain: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 2: GENERACI√ìN DIRECTA CON API DE OLLAMA (m√°s r√°pido)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_with_raw_ollama(self, question: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando directamente la API HTTP de Ollama.\n",
        "        Este m√©todo es m√°s r√°pido pero menos robusto que el m√©todo LangChain.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "            context: Contexto recuperado de la base de conocimiento\n",
        "\n",
        "        Returns:\n",
        "            Texto de respuesta generado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Formatear el prompt con contexto y pregunta\n",
        "            formatted_prompt = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Asume que eres un experto en econom√≠a. Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa, con un lenguaje acad√©mico y formal. Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "Pregunta: {question}\n",
        "\"\"\"\n",
        "\n",
        "            # Configurar la llamada HTTP a Ollama\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            payload = {\n",
        "                \"model\": self.ollama_model,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"stream\": False,  # No usar streaming para simplificar\n",
        "                \"temperature\": 0.1,  # Consistente con el otro m√©todo\n",
        "                \"num_predict\": 512  # N√∫mero m√°ximo de tokens\n",
        "            }\n",
        "\n",
        "            # Realizar la llamada API HTTP directa\n",
        "            print(\"Llamando a la API de Ollama con requests...\")\n",
        "            response = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            # Procesar la respuesta\n",
        "            if response.status_code == 200:\n",
        "                respuesta_json = response.json()\n",
        "                respuesta = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "                return respuesta\n",
        "            else:\n",
        "                return f\"Error en la API de Ollama: C√≥digo {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error llamando directamente a Ollama: {str(e)}\")\n",
        "            # Si falla, devolver mensaje de error y sugerir usar el m√©todo est√°ndar\n",
        "            return \"Error al usar Ollama directamente. Intenta desactivar 'Usar Ollama directo'.\""
      ],
      "metadata": {
        "id": "_vM2X8Q3Pljr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 7: FUNCI√ìN DE PROCESAMIENTO DE RESPUESTAS\n",
        "\n",
        "def process_response(user_input: str, chat_history, files, use_direct_ollama=True):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y genera una respuesta utilizando el sistema RAG.\n",
        "    Esta funci√≥n coordina todo el proceso de consulta desde la entrada hasta la respuesta.\n",
        "\n",
        "    Args:\n",
        "        user_input: Pregunta o instrucci√≥n del usuario\n",
        "        chat_history: Historial de chat actual\n",
        "        files: Archivos cargados por el usuario\n",
        "        use_direct_ollama: Si es True, usa la API directa de Ollama; si es False, usa LangChain\n",
        "\n",
        "    Returns:\n",
        "        Historial de chat actualizado con la nueva pregunta y respuesta\n",
        "    \"\"\"\n",
        "    # Ignorar entradas vac√≠as\n",
        "    if not user_input.strip():\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # PASO 1: Inicializaci√≥n si es necesario\n",
        "        if not rag_system.is_initialized:\n",
        "            rag_system.initialize_system()\n",
        "\n",
        "        # PASO 2: Procesar documentos si hay archivos nuevos\n",
        "        if files:\n",
        "            rag_system.process_documents(files)\n",
        "\n",
        "        # Verificar que haya documentos procesados\n",
        "        if rag_system.vector_store is None:\n",
        "            answer = \"Por favor, carga algunos documentos antes de hacer preguntas.\"\n",
        "            chat_history.append((user_input, answer))\n",
        "            return chat_history\n",
        "\n",
        "        # PASO 3: Recuperar documentos relevantes para la consulta\n",
        "        print(\"üîç Buscando documentos relevantes...\")\n",
        "        documents = rag_system.vector_store.similarity_search(user_input, k=6)\n",
        "        # Unir el contenido de los documentos como contexto\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "        # PASO 4: Generar respuesta seg√∫n el m√©todo seleccionado\n",
        "        if use_direct_ollama:\n",
        "            # --------- M√âTODO DIRECTO (M√ÅS R√ÅPIDO) ---------\n",
        "            try:\n",
        "                print(\"üöÄ Usando m√©todo directo de Ollama...\")\n",
        "                answer = rag_system.generate_with_raw_ollama(user_input, context)\n",
        "\n",
        "                # Implementaci√≥n de fallback: si hay error, usar m√©todo est√°ndar\n",
        "                if answer.startswith(\"Error\"):\n",
        "                    print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                    response = rag_system.generate_response(user_input)\n",
        "                    answer = response['answer']\n",
        "\n",
        "                    # A√±adir informaci√≥n de fuentes\n",
        "                    sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                    if sources:\n",
        "                        answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "            except Exception as ollama_error:\n",
        "                # Manejo de error: si falla el m√©todo directo, usar el est√°ndar\n",
        "                print(f\"‚ùå Error en m√©todo directo: {str(ollama_error)}\")\n",
        "                print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                response = rag_system.generate_response(user_input)\n",
        "                answer = response['answer']\n",
        "\n",
        "                # A√±adir informaci√≥n de fuentes\n",
        "                sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                if sources:\n",
        "                    answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "        else:\n",
        "            # --------- M√âTODO EST√ÅNDAR (M√ÅS ROBUSTO) ---------\n",
        "            print(\"üîÑ Usando m√©todo est√°ndar de LangChain...\")\n",
        "            response = rag_system.generate_response(user_input)\n",
        "            answer = response['answer']\n",
        "\n",
        "            # A√±adir informaci√≥n de fuentes\n",
        "            sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "            if sources:\n",
        "                answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "\n",
        "        # PASO 5: Actualizar el historial de chat y retornar\n",
        "        chat_history.append((user_input, answer))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Manejo de errores generales\n",
        "        error_message = f\"Lo siento, ocurri√≥ un error: {str(e)}\"\n",
        "        print(f\"‚ùå Error general en process_response: {str(e)}\")\n",
        "        chat_history.append((user_input, error_message))\n",
        "        return chat_history\n",
        "\n"
      ],
      "metadata": {
        "id": "jg2NGjPNTAR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 8: INICIALIZACI√ìN DEL SISTEMA\n",
        "\n",
        "# Crear la instancia del sistema RAG\n",
        "print(\"üîß Inicializando sistema RAG con Ollama...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ Sistema RAG creado correctamente\")"
      ],
      "metadata": {
        "id": "PZRWgtL0Tm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a39ac3b-e703-4f2b-ea5a-84d0f3e29afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Inicializando sistema RAG con Ollama...\n",
            "‚úÖ Sistema RAG creado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 9: INTERFAZ GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la interfaz web con Gradio\n",
        "print(\"üåê Creando interfaz Gradio...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Encabezado\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #2d333a;\">üìö RAG Assistant con Ollama</h1>\n",
        "            <p style=\"color: #4a5568;\">\n",
        "                Asistente IA para an√°lisis y consulta de documentos usando Ollama\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Secci√≥n de carga de archivos y configuraci√≥n\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Selector de archivos\n",
        "            files = gr.Files(\n",
        "                label=\"Carga tus documentos\",\n",
        "                file_types=SUPPORTED_FORMATS,\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "\n",
        "            # Opci√≥n para seleccionar m√©todo de generaci√≥n\n",
        "            use_direct_ollama = gr.Checkbox(\n",
        "                label=\"Usar Ollama directo (m√°s r√°pido)\",\n",
        "                value=False,  # Falso por defecto para mayor estabilidad\n",
        "                info=\"Hace llamadas directas a la API de Ollama para respuestas m√°s r√°pidas.\"\n",
        "            )\n",
        "\n",
        "            # Informaci√≥n sobre formatos soportados\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"font-size: 0.9em; color: #666; margin-top: 0.5em;\">\n",
        "                    Formatos soportados: PDF, DOCX, CSV, TXT\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Interfaz de chat\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "        scale=2\n",
        "    )\n",
        "\n",
        "    # √Årea de entrada de texto y bot√≥n de limpieza\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            placeholder=\"üí≠ Pregunta cualquier cosa sobre tus documentos...\",\n",
        "            show_label=False,\n",
        "            container=False,\n",
        "            scale=8,\n",
        "            autofocus=True\n",
        "        )\n",
        "        clear = gr.Button(\"üóëÔ∏è Limpiar\", size=\"sm\", scale=1)\n",
        "\n",
        "    # Secci√≥n de instrucciones\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
        "            <h3 style=\"color: #2d333a; margin-bottom: 10px;\">üîç C√≥mo usar:</h3>\n",
        "            <ol style=\"color: #666; margin-left: 20px;\">\n",
        "                <li>Carga uno o m√°s documentos (PDF, DOCX, CSV, o TXT)</li>\n",
        "                <li>Espera a que los documentos sean procesados</li>\n",
        "                <li>Haz preguntas sobre el contenido de tus documentos</li>\n",
        "                <li>Activa \"Usar Ollama directo\" para respuestas m√°s r√°pidas (desact√≠valo si hay errores)</li>\n",
        "            </ol>\n",
        "            <p style=\"color: #666; font-style: italic; margin-top: 10px;\">\n",
        "                Nota: La primera respuesta puede tardar un poco. Desde la segunda respuesta es m√°s r√°pido.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Pie de p√°gina con informaci√≥n t√©cnica y cr√©ditos\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 20px auto; padding: 20px;\n",
        "                    background-color: #f8f9fa; border-radius: 10px;\">\n",
        "            <div style=\"margin-bottom: 15px;\">\n",
        "                <h3 style=\"color: #2d333a;\">‚ö° Sobre este asistente</h3>\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Esta aplicaci√≥n utiliza tecnolog√≠a RAG (Retrieval Augmented Generation) combinando:\n",
        "                </p>\n",
        "                <ul style=\"list-style: none; color: #666; font-size: 14px;\">\n",
        "                    <li>üîπ Motor LLM: Ollama con Llama2</li>\n",
        "                    <li>üîπ Embeddings: multilingual-e5-small</li>\n",
        "                    <li>üîπ Base de datos vectorial: FAISS</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div style=\"border-top: 1px solid #ddd; padding-top: 15px;\">\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Proyecto examen final curso Inteligencia Artificial Aplicada a la Econom√≠a - Universidad de los Andes<br>\n",
        "\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # --------- FUNCIONES DE CONTROL DE LA INTERFAZ ---------\n",
        "    # Funci√≥n para limpiar el contexto y reiniciar\n",
        "    def clear_context():\n",
        "        # Eliminar la base de conocimiento y reiniciar el registro de archivos\n",
        "        rag_system.vector_store = None\n",
        "        rag_system.processed_files.clear()\n",
        "        return None\n",
        "\n",
        "    # Conectar eventos de la interfaz con funciones\n",
        "    message.submit(process_response, [message, chatbot, files, use_direct_ollama], [chatbot])\n",
        "    clear.click(clear_context, None, chatbot)\n",
        "\n",
        "# Lanzar la interfaz web\n",
        "print(\"üöÄ Lanzando interfaz Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "5Yk--0McTuJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db833182-c529-4ce4-ae8d-d3e3b50ee532"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Creando interfaz Gradio...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-50-43c02599d002>:42: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "<ipython-input-50-43c02599d002>:42: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Lanzando interfaz Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b99edcfb7b341d7112.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b99edcfb7b341d7112.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/d9657f5034c853c4a36bd7fe10238cc76f0b6285d38ddff2710ca39d70259130/3.2 Nunn - The Importance of History for Economic Development.docx\n",
            "‚úÖ Archivo cargado exitosamente: /tmp/gradio/d9657f5034c853c4a36bd7fe10238cc76f0b6285d38ddff2710ca39d70259130/3.2 Nunn - The Importance of History for Economic Development.docx\n",
            "‚úÇÔ∏è Dividiendo documentos en fragmentos...\n",
            "üß© Documentos divididos en 216 fragmentos\n",
            "üîç Vectorizando fragmentos...\n",
            "‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\n",
            "‚úÖ Procesamiento completado: 1 documentos a√±adidos a la base de conocimiento\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: De qu√© se trata el texto?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n",
            "üìÑ Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "‚úÖ Archivo cargado exitosamente: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "‚úÇÔ∏è Dividiendo documentos en fragmentos...\n",
            "üß© Documentos divididos en 214 fragmentos\n",
            "üîç Vectorizando fragmentos...\n",
            "‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\n",
            "‚úÖ Procesamiento completado: 61 documentos a√±adidos a la base de conocimiento\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: Cu√°l es la conclusi√≥n del texto?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n",
            "üìÑ Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "‚úÖ Archivo cargado exitosamente: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "‚úÇÔ∏è Dividiendo documentos en fragmentos...\n",
            "üß© Documentos divididos en 214 fragmentos\n",
            "üîç Vectorizando fragmentos...\n",
            "‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\n",
            "‚úÖ Procesamiento completado: 61 documentos a√±adidos a la base de conocimiento\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: Cu√°les son los resultados de los experimentos mencionados en el art√≠culo?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n",
            "üìÑ Procesando 1 documento(s)...\n",
            "‚ÑπÔ∏è No hay documentos nuevos para procesar\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: Qui√©nes son los autores del texto?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n",
            "üìÑ Procesando 1 documento(s)...\n",
            "‚ÑπÔ∏è No hay documentos nuevos para procesar\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: Esos experimentos son replicables en otros pa√≠ses en desarrollo?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n",
            "üìÑ Procesando 1 documento(s)...\n",
            "‚ÑπÔ∏è No hay documentos nuevos para procesar\n",
            "üîç Buscando documentos relevantes...\n",
            "üîÑ Usando m√©todo est√°ndar de LangChain...\n",
            "‚ùì Procesando pregunta: Cu√°les son las conclusiones del texto?\n",
            "‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\n"
          ]
        }
      ]
    }
  ]
}