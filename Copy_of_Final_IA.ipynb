{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurapabon03/Final-IA-/blob/main/Copy_of_Final_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parcial Final Inteligencia Artificial\n",
        "\n",
        "Ángela Sofia Torres, Juan David Saldaña Rivera, Laura Pabón, Dario Montoya, Lucas Alvarado"
      ],
      "metadata": {
        "id": "26NxRaHnuLYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 1: INSTALACIÓN DE DEPENDENCIAS\n",
        "# Estas líneas instalan las bibliotecas necesarias para el funcionamiento del sistema\n",
        "# Se ejecutan solamente en entornos como Google Colab o Jupyter Notebooks\n",
        "\n",
        "# Instalación de bibliotecas esenciales para RAG\n",
        "!pip install -q langchain langchain_community sentence-transformers pypdf python-docx docx2txt unstructured faiss-cpu gradio\n",
        "!pip install -q chromadb requests"
      ],
      "metadata": {
        "id": "IX_VoOPvKg9d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 2: CONFIGURACIÓN DE OLLAMA\n",
        "# Ollama es una herramienta que permite ejecutar modelos de lenguaje localmente con menor uso de recursos que otros sistemas\n",
        "\n",
        "# Instalación de Ollama\n",
        "print(\"Instalando Ollama para inferencia rápida...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Iniciar el servicio de Ollama en segundo plano\n",
        "print(\"\\nIniciando servidor Ollama...\")\n",
        "!pkill ollama || true  # Detener cualquier instancia anterior\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar en segundo plano\n",
        "\n",
        "# Dar tiempo al servidor para inicializar\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama esté listo...\")\n",
        "time.sleep(15)  # Esperar 15 segundos\n",
        "\n",
        "# Verificar que el servidor esté activo\n",
        "print(\"\\nVerificando que el servidor Ollama esté respondiendo...\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no está respondiendo\"\n",
        "\n",
        "# Descargar el modelo LLM que usaremos (Llama2)\n",
        "print(\"\\nDescargando modelo llama2 desde Ollama...\")\n",
        "!ollama pull llama2"
      ],
      "metadata": {
        "id": "nyJWf9CNKpA4",
        "outputId": "cfb1ff30-8562-4777-e5c2-7c19a7b9eb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Ollama para inferencia rápida...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "Iniciando servidor Ollama...\n",
            "Esperando a que el servidor Ollama esté listo...\n",
            "\n",
            "Verificando que el servidor Ollama esté respondiendo...\n",
            "{\"models\":[{\"name\":\"llama2:latest\",\"model\":\"llama2:latest\",\"modified_at\":\"2025-05-15T20:54:49.744415211Z\",\"size\":3826793677,\"digest\":\"78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"7B\",\"quantization_level\":\"Q4_0\"}}]}\n",
            "Descargando modelo llama2 desde Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SECCIÓN 3: IMPORTACIÓN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "#SECCIÓN 3: IMPORTACIÓN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "from langchain_community.llms import Ollama  # Integración LangChain-Ollama\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "lTOu6XV4MqlJ",
        "outputId": "b938150a-5920-4fda-9aa3-5a636b322c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Aquí seleccionas tu archivo kaggle.json\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Crear la carpeta .kaggle\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Mover kaggle.json allí\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "\n",
        "# Cambiar permisos\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "!kaggle datasets download -d nitindatta/finance-data\n",
        "\n",
        "!unzip -o -q finance-data.zip -d finance-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "XJM7cBWDKQMw",
        "outputId": "1868c6b9-26be-42e2-a059-0a5bbb602b68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5cec8cb-de4d-4f0d-96a7-00b819b1a035\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5cec8cb-de4d-4f0d-96a7-00b819b1a035\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/nitindatta/finance-data\n",
            "License(s): unknown\n",
            "finance-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1.1: Configurar kaggle.json (ya subido)\n",
        "import os\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Paso 2: Descargar el dataset\n",
        "!kaggle datasets download -d nitindatta/finance-data\n",
        "!unzip -o finance-data.zip -d finance-data\n",
        "\n",
        "# Paso 3: Cargar los datos\n",
        "import pandas as pd\n",
        "\n",
        "# Explorar archivos disponibles\n",
        "print(\"Archivos disponibles:\", os.listdir(\"finance-data\"))\n",
        "\n",
        "# Verificar columnas disponibles para escoger el CSV adecuado\n",
        "df = pd.read_csv(\"finance-data/Finance_data.csv\")  # Ajustado según nombre real\n",
        "print(\"Columnas disponibles:\", df.columns.tolist())\n",
        "\n",
        "# (Si 'date' y 'headline' no existen, muestra los primeros registros)\n",
        "print(df.head(3))\n",
        "\n",
        "# Si existen columnas 'date' y 'headline', procesarlas\n",
        "if 'date' in df.columns and 'headline' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m')\n",
        "    df = df.dropna(subset=['date', 'headline'])\n",
        "    print(df[['date', 'headline']].head())\n",
        "else:\n",
        "    print(\"⚠️ Las columnas 'date' y/o 'headline' no están presentes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eu8eDzy3Bpm",
        "outputId": "fcd595ba-b70e-488d-b9ec-681d097d353f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/nitindatta/finance-data\n",
            "License(s): unknown\n",
            "finance-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  finance-data.zip\n",
            "  inflating: finance-data/Finance_data.csv  \n",
            "  inflating: finance-data/Original_data.csv  \n",
            "Archivos disponibles: ['Finance_data.csv', 'Original_data.csv']\n",
            "Columnas disponibles: ['gender', 'age', 'Investment_Avenues', 'Mutual_Funds', 'Equity_Market', 'Debentures', 'Government_Bonds', 'Fixed_Deposits', 'PPF', 'Gold', 'Stock_Marktet', 'Factor', 'Objective', 'Purpose', 'Duration', 'Invest_Monitor', 'Expect', 'Avenue', 'What are your savings objectives?', 'Reason_Equity', 'Reason_Mutual', 'Reason_Bonds', 'Reason_FD', 'Source']\n",
            "   gender  age Investment_Avenues  Mutual_Funds  Equity_Market  Debentures  \\\n",
            "0  Female   34                Yes             1              2           5   \n",
            "1  Female   23                Yes             4              3           2   \n",
            "2    Male   30                Yes             3              6           4   \n",
            "\n",
            "   Government_Bonds  Fixed_Deposits  PPF  Gold  ...           Duration  \\\n",
            "0                 3               7    6     4  ...          1-3 years   \n",
            "1                 1               5    6     7  ...  More than 5 years   \n",
            "2                 2               5    1     7  ...          3-5 years   \n",
            "\n",
            "  Invest_Monitor   Expect       Avenue What are your savings objectives?  \\\n",
            "0        Monthly  20%-30%  Mutual Fund                   Retirement Plan   \n",
            "1         Weekly  20%-30%  Mutual Fund                       Health Care   \n",
            "2          Daily  20%-30%       Equity                   Retirement Plan   \n",
            "\n",
            "          Reason_Equity   Reason_Mutual     Reason_Bonds            Reason_FD  \\\n",
            "0  Capital Appreciation  Better Returns  Safe Investment        Fixed Returns   \n",
            "1              Dividend  Better Returns  Safe Investment  High Interest Rates   \n",
            "2  Capital Appreciation    Tax Benefits  Assured Returns        Fixed Returns   \n",
            "\n",
            "                     Source  \n",
            "0  Newspapers and Magazines  \n",
            "1     Financial Consultants  \n",
            "2                Television  \n",
            "\n",
            "[3 rows x 24 columns]\n",
            "⚠️ Las columnas 'date' y/o 'headline' no están presentes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 4: CONFIGURACIÓN BÁSICA\n",
        "\n",
        "# Configuración de logs para monitoreo y depuración\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constantes para configuración del sistema\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".doc\", \".csv\", \".txt\"]  # Formatos soportados\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"  # Modelo para codificación semántica\n",
        "OLLAMA_MODEL = \"llama2\"  # Modelo LLM local\n"
      ],
      "metadata": {
        "id": "NgJrarcKOpDa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 5: CLASE PARA CARGA DE DOCUMENTOS\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Cargador unificado de documentos que soporta múltiples formatos.\n",
        "    Esta clase selecciona el cargador adecuado según la extensión del archivo.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> List:\n",
        "        \"\"\"\n",
        "        Carga un archivo basado en su extensión y devuelve los documentos procesados.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo a cargar\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos procesados con sus metadatos\n",
        "        \"\"\"\n",
        "        print(f\"Cargando archivo: {file_path}\")\n",
        "        ext = os.path.splitext(file_path)[1].lower()  # Obtener extensión del archivo\n",
        "\n",
        "        try:\n",
        "            # Seleccionar el cargador apropiado según el tipo de archivo\n",
        "            if ext == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)  # Para archivos PDF\n",
        "            elif ext in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)  # Para documentos Word\n",
        "            elif ext == '.csv':\n",
        "                loader = CSVLoader(file_path)  # Para archivos CSV\n",
        "            else:  # Para txt y otros formatos de texto\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "            # Ejecutar la carga del documento\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Enriquecer con metadatos para mejorar la recuperación y visualización\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    'title': os.path.basename(file_path),  # Nombre del archivo\n",
        "                    'type': 'document',  # Tipo de contenido\n",
        "                    'format': ext[1:],  # Formato sin el punto inicial\n",
        "                    'language': 'auto'  # Idioma (auto-detectado)\n",
        "                })\n",
        "\n",
        "            print(f\"✅ Archivo cargado exitosamente: {file_path}\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error al cargar {file_path}: {str(e)}\")\n",
        "            raise  # Re-lanzar la excepción para manejo superior"
      ],
      "metadata": {
        "id": "S0IV8BtROq-l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 6: CLASE PRINCIPAL DEL SISTEMA RAG\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Sistema RAG completo con Ollama para consulta de documentos.\n",
        "\n",
        "    Esta clase implementa todo el flujo de trabajo RAG:\n",
        "    1. Carga y procesamiento de documentos\n",
        "    2. Generación de embeddings y almacenamiento vectorial\n",
        "    3. Recuperación de contexto relevante\n",
        "    4. Generación de respuestas mediante LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL, ollama_model: str = OLLAMA_MODEL):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema RAG con los modelos especificados.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Modelo para generar embeddings (representaciones vectoriales)\n",
        "            ollama_model: Modelo de lenguaje a utilizar con Ollama\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.ollama_model = ollama_model\n",
        "        self.embeddings = None  # Se inicializará posteriormente\n",
        "        self.vector_store = None  # Base de datos vectorial\n",
        "        self.qa_chain = None  # Cadena de pregunta-respuesta\n",
        "        self.is_initialized = False  # Flag de inicialización\n",
        "        self.processed_files = set()  # Conjunto para evitar procesar archivos duplicados\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"\n",
        "        Inicializa los componentes del sistema RAG:\n",
        "        - Modelo de embeddings\n",
        "        - Conexión con Ollama\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🚀 Inicializando sistema RAG con Ollama...\")\n",
        "\n",
        "            # Inicializar el modelo de embeddings (usando CPU o GPU si está disponible)\n",
        "            print(\"📊 Cargando modelo de embeddings...\")\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=self.embedding_model,\n",
        "                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "                encode_kwargs={'normalize_embeddings': True}  # Normalización para mejor búsqueda\n",
        "            )\n",
        "\n",
        "            # Verificación de salud de Ollama - reintento si no responde\n",
        "            try:\n",
        "                response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "                if response.status_code != 200:\n",
        "                    print(\"⚠️ Advertencia: Ollama no está respondiendo correctamente. Reintentando inicialización...\")\n",
        "                    time.sleep(5)\n",
        "                    # Reinicio de emergencia del servicio Ollama\n",
        "                    subprocess.run(\"pkill ollama || true\", shell=True)\n",
        "                    subprocess.run(\"nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &\", shell=True)\n",
        "                    time.sleep(15)  # Esperar a que reinicie\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Advertencia al verificar Ollama: {str(e)}\")\n",
        "\n",
        "            # Configurar Ollama como modelo de lenguaje mediante LangChain\n",
        "            print(\"🧠 Configurando Ollama como LLM...\")\n",
        "            self.llm = Ollama(\n",
        "                model=self.ollama_model,\n",
        "                temperature=0.1,  # Temperatura baja para respuestas más deterministas\n",
        "                num_predict=512  # Máximo de tokens a generar\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True  # Marcar como inicializado\n",
        "            print(\"✅ Sistema RAG inicializado correctamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error durante la inicialización: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_documents(self, files: List[tempfile._TemporaryFileWrapper]) -> None:\n",
        "        \"\"\"\n",
        "        Procesa documentos cargados y actualiza la base de datos vectorial.\n",
        "\n",
        "        Args:\n",
        "            files: Lista de archivos temporales cargados por el usuario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            documents = []  # Lista para almacenar todos los documentos\n",
        "            new_files = []  # Seguimiento de archivos nuevos procesados\n",
        "\n",
        "            print(f\"📄 Procesando {len(files)} documento(s)...\")\n",
        "\n",
        "            # Filtrar y procesar solo archivos que no se han procesado antes\n",
        "            for file in files:\n",
        "                if file.name not in self.processed_files:\n",
        "                    docs = DocumentLoader.load_file(file.name)  # Cargar el archivo\n",
        "                    documents.extend(docs)  # Añadir documentos a la lista\n",
        "                    new_files.append(file.name)  # Registrar como nuevo\n",
        "                    self.processed_files.add(file.name)  # Marcar como procesado\n",
        "\n",
        "            # Si no hay archivos nuevos, terminar\n",
        "            if not new_files:\n",
        "                print(\"ℹ️ No hay documentos nuevos para procesar\")\n",
        "                return\n",
        "\n",
        "            # Verificar que se hayan cargado documentos\n",
        "            if not documents:\n",
        "                raise ValueError(\"No se pudieron cargar documentos.\")\n",
        "\n",
        "            # --------- DIVISIÓN DE DOCUMENTOS ---------\n",
        "            # Dividir documentos en fragmentos más pequeños para procesamiento eficiente\n",
        "            print(\"✂️ Dividiendo documentos en fragmentos...\")\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=800,  # Tamaño objetivo de cada fragmento (en caracteres)\n",
        "                chunk_overlap=200,  # Superposición entre fragmentos para mantener contexto\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separación\n",
        "                length_function=len  # Función para medir longitud\n",
        "            )\n",
        "\n",
        "            # Aplicar la división a todos los documentos\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            print(f\"🧩 Documentos divididos en {len(chunks)} fragmentos\")\n",
        "\n",
        "            # --------- VECTORIZACIÓN Y ALMACENAMIENTO ---------\n",
        "            # Crear o actualizar la base de datos vectorial con los nuevos fragmentos\n",
        "            print(\"🔍 Vectorizando fragmentos...\")\n",
        "            if self.vector_store is None:\n",
        "                # Primera carga: crear nueva base de datos vectorial\n",
        "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "            else:\n",
        "                # Carga adicional: añadir a la base de datos existente\n",
        "                self.vector_store.add_documents(chunks)\n",
        "\n",
        "            # --------- CONFIGURACIÓN DE PROMPT ---------\n",
        "            # Definir la plantilla de prompt para el LLM\n",
        "            prompt_template = \"\"\"\n",
        "            Contexto: {context}\n",
        "\n",
        "            Basándote únicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "            Si la información no está en el contexto, indícalo explícitamente.\n",
        "\n",
        "            Pregunta: {question}\n",
        "            \"\"\"\n",
        "\n",
        "            # Crear objeto de prompt con variables\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template,\n",
        "                input_variables=[\"context\", \"question\"]  # Variables a rellenar\n",
        "            )\n",
        "\n",
        "            # --------- CONFIGURACIÓN DE CADENA QA ---------\n",
        "            # Inicializar la cadena de pregunta-respuesta con Ollama\n",
        "            print(\"⚙️ Configurando cadena de pregunta-respuesta con Ollama...\")\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,  # Modelo de lenguaje\n",
        "                chain_type=\"stuff\",  # Tipo de cadena (insertar todo el contexto de una vez)\n",
        "                retriever=self.vector_store.as_retriever(\n",
        "                    search_kwargs={\"k\": 6}  # Recuperar los 6 fragmentos más relevantes\n",
        "                ),\n",
        "                return_source_documents=True,  # Devolver documentos fuente para citas\n",
        "                chain_type_kwargs={\"prompt\": PROMPT}  # Usar nuestro prompt personalizado\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Procesamiento completado: {len(documents)} documentos añadidos a la base de conocimiento\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error procesando documentos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # MÉTODO 1: GENERACIÓN MEDIANTE LANGCHAIN (más robusto)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_response(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Genera una respuesta utilizando el framework LangChain.\n",
        "        Este método es más robusto y estructurado, con mejor manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la respuesta y fuentes utilizadas\n",
        "        \"\"\"\n",
        "        # Verificar que el sistema esté inicializado\n",
        "        if not self.is_initialized or self.vector_store is None:\n",
        "            return {\n",
        "                'answer': \"Por favor, carga algunos documentos antes de hacer preguntas.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"❓ Procesando pregunta: {question}\")\n",
        "\n",
        "            # Ejecutar la cadena QA con LangChain y Ollama\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Preparar la respuesta estructurada\n",
        "            response = {\n",
        "                'answer': result['result'],  # Respuesta generada\n",
        "                'sources': []  # Lista para fuentes\n",
        "            }\n",
        "\n",
        "            # Añadir información sobre las fuentes utilizadas\n",
        "            for doc in result['source_documents']:\n",
        "                source = {\n",
        "                    'title': doc.metadata.get('title', 'Desconocido'),\n",
        "                    'content': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    'metadata': doc.metadata\n",
        "                }\n",
        "                response['sources'].append(source)\n",
        "\n",
        "            print(\"✅ Respuesta generada con éxito usando el método LangChain\")\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error generando respuesta con LangChain: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # MÉTODO 2: GENERACIÓN DIRECTA CON API DE OLLAMA (más rápido)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_with_raw_ollama(self, question: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando directamente la API HTTP de Ollama.\n",
        "        Este método es más rápido pero menos robusto que el método LangChain.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "            context: Contexto recuperado de la base de conocimiento\n",
        "\n",
        "        Returns:\n",
        "            Texto de respuesta generado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Formatear el prompt con contexto y pregunta\n",
        "            formatted_prompt = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Basándote únicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "Si la información no está en el contexto, indícalo explícitamente.\n",
        "\n",
        "Pregunta: {question}\n",
        "\"\"\"\n",
        "\n",
        "            # Configurar la llamada HTTP a Ollama\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            payload = {\n",
        "                \"model\": self.ollama_model,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"stream\": False,  # No usar streaming para simplificar\n",
        "                \"temperature\": 0.1,  # Consistente con el otro método\n",
        "                \"num_predict\": 512  # Número máximo de tokens\n",
        "            }\n",
        "\n",
        "            # Realizar la llamada API HTTP directa\n",
        "            print(\"Llamando a la API de Ollama con requests...\")\n",
        "            response = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            # Procesar la respuesta\n",
        "            if response.status_code == 200:\n",
        "                respuesta_json = response.json()\n",
        "                respuesta = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "                return respuesta\n",
        "            else:\n",
        "                return f\"Error en la API de Ollama: Código {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error llamando directamente a Ollama: {str(e)}\")\n",
        "            # Si falla, devolver mensaje de error y sugerir usar el método estándar\n",
        "            return \"Error al usar Ollama directamente. Intenta desactivar 'Usar Ollama directo'.\""
      ],
      "metadata": {
        "id": "_vM2X8Q3Pljr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 7: FUNCIÓN DE PROCESAMIENTO DE RESPUESTAS\n",
        "\n",
        "def process_response(user_input: str, chat_history, files, use_direct_ollama=True):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y genera una respuesta utilizando el sistema RAG.\n",
        "    Esta función coordina todo el proceso de consulta desde la entrada hasta la respuesta.\n",
        "\n",
        "    Args:\n",
        "        user_input: Pregunta o instrucción del usuario\n",
        "        chat_history: Historial de chat actual\n",
        "        files: Archivos cargados por el usuario\n",
        "        use_direct_ollama: Si es True, usa la API directa de Ollama; si es False, usa LangChain\n",
        "\n",
        "    Returns:\n",
        "        Historial de chat actualizado con la nueva pregunta y respuesta\n",
        "    \"\"\"\n",
        "    # Ignorar entradas vacías\n",
        "    if not user_input.strip():\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # PASO 1: Inicialización si es necesario\n",
        "        if not rag_system.is_initialized:\n",
        "            rag_system.initialize_system()\n",
        "\n",
        "        # PASO 2: Procesar documentos si hay archivos nuevos\n",
        "        if files:\n",
        "            rag_system.process_documents(files)\n",
        "\n",
        "        # Verificar que haya documentos procesados\n",
        "        if rag_system.vector_store is None:\n",
        "            answer = \"Por favor, carga algunos documentos antes de hacer preguntas.\"\n",
        "            chat_history.append((user_input, answer))\n",
        "            return chat_history\n",
        "\n",
        "        # PASO 3: Recuperar documentos relevantes para la consulta\n",
        "        print(\"🔍 Buscando documentos relevantes...\")\n",
        "        documents = rag_system.vector_store.similarity_search(user_input, k=6)\n",
        "        # Unir el contenido de los documentos como contexto\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "        # PASO 4: Generar respuesta según el método seleccionado\n",
        "        if use_direct_ollama:\n",
        "            # --------- MÉTODO DIRECTO (MÁS RÁPIDO) ---------\n",
        "            try:\n",
        "                print(\"🚀 Usando método directo de Ollama...\")\n",
        "                answer = rag_system.generate_with_raw_ollama(user_input, context)\n",
        "\n",
        "                # Implementación de fallback: si hay error, usar método estándar\n",
        "                if answer.startswith(\"Error\"):\n",
        "                    print(\"⚠️ Retrocediendo al método estándar...\")\n",
        "                    response = rag_system.generate_response(user_input)\n",
        "                    answer = response['answer']\n",
        "\n",
        "                    # Añadir información de fuentes\n",
        "                    sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                    if sources:\n",
        "                        answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "            except Exception as ollama_error:\n",
        "                # Manejo de error: si falla el método directo, usar el estándar\n",
        "                print(f\"❌ Error en método directo: {str(ollama_error)}\")\n",
        "                print(\"⚠️ Retrocediendo al método estándar...\")\n",
        "                response = rag_system.generate_response(user_input)\n",
        "                answer = response['answer']\n",
        "\n",
        "                # Añadir información de fuentes\n",
        "                sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                if sources:\n",
        "                    answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "        else:\n",
        "            # --------- MÉTODO ESTÁNDAR (MÁS ROBUSTO) ---------\n",
        "            print(\"🔄 Usando método estándar de LangChain...\")\n",
        "            response = rag_system.generate_response(user_input)\n",
        "            answer = response['answer']\n",
        "\n",
        "            # Añadir información de fuentes\n",
        "            sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "            if sources:\n",
        "                answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "\n",
        "        # PASO 5: Actualizar el historial de chat y retornar\n",
        "        chat_history.append((user_input, answer))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Manejo de errores generales\n",
        "        error_message = f\"Lo siento, ocurrió un error: {str(e)}\"\n",
        "        print(f\"❌ Error general en process_response: {str(e)}\")\n",
        "        chat_history.append((user_input, error_message))\n",
        "        return chat_history\n",
        "\n"
      ],
      "metadata": {
        "id": "jg2NGjPNTAR0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 8: INICIALIZACIÓN DEL SISTEMA\n",
        "\n",
        "# Crear la instancia del sistema RAG\n",
        "print(\"🔧 Inicializando sistema RAG con Ollama...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"✅ Sistema RAG creado correctamente\")"
      ],
      "metadata": {
        "id": "PZRWgtL0Tm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0005dbb8-a01a-4c41-9df4-c0108cc509b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Inicializando sistema RAG con Ollama...\n",
            "✅ Sistema RAG creado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SD5tNWHnVlIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 9: INTERFAZ GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la interfaz web con Gradio\n",
        "print(\"🌐 Creando interfaz Gradio...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Encabezado\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #2d333a;\">📚 RAG Assistant con Ollama</h1>\n",
        "            <p style=\"color: #4a5568;\">\n",
        "                Asistente IA para análisis y consulta de documentos usando Ollama\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Sección de carga de archivos y configuración\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Selector de archivos\n",
        "            files = gr.Files(\n",
        "                label=\"Carga tus documentos\",\n",
        "                file_types=SUPPORTED_FORMATS,\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "\n",
        "            # Opción para seleccionar método de generación\n",
        "            use_direct_ollama = gr.Checkbox(\n",
        "                label=\"Usar Ollama directo (más rápido)\",\n",
        "                value=False,  # Falso por defecto para mayor estabilidad\n",
        "                info=\"Hace llamadas directas a la API de Ollama para respuestas más rápidas.\"\n",
        "            )\n",
        "\n",
        "            # Información sobre formatos soportados\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"font-size: 0.9em; color: #666; margin-top: 0.5em;\">\n",
        "                    Formatos soportados: PDF, DOCX, CSV, TXT\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Interfaz de chat\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "        scale=2\n",
        "    )\n",
        "\n",
        "    # Área de entrada de texto y botón de limpieza\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            placeholder=\"💭 Pregunta cualquier cosa sobre tus documentos...\",\n",
        "            show_label=False,\n",
        "            container=False,\n",
        "            scale=8,\n",
        "            autofocus=True\n",
        "        )\n",
        "        clear = gr.Button(\"🗑️ Limpiar\", size=\"sm\", scale=1)\n",
        "\n",
        "    # Sección de instrucciones\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
        "            <h3 style=\"color: #2d333a; margin-bottom: 10px;\">🔍 Cómo usar:</h3>\n",
        "            <ol style=\"color: #666; margin-left: 20px;\">\n",
        "                <li>Carga uno o más documentos (PDF, DOCX, CSV, o TXT)</li>\n",
        "                <li>Espera a que los documentos sean procesados</li>\n",
        "                <li>Haz preguntas sobre el contenido de tus documentos</li>\n",
        "                <li>Activa \"Usar Ollama directo\" para respuestas más rápidas (desactívalo si hay errores)</li>\n",
        "            </ol>\n",
        "            <p style=\"color: #666; font-style: italic; margin-top: 10px;\">\n",
        "                Nota: La primera respuesta puede tardar un poco. Desde la segunda respuesta es más rápido.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Pie de página con información técnica y créditos\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 20px auto; padding: 20px;\n",
        "                    background-color: #f8f9fa; border-radius: 10px;\">\n",
        "            <div style=\"margin-bottom: 15px;\">\n",
        "                <h3 style=\"color: #2d333a;\">⚡ Sobre este asistente</h3>\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Esta aplicación utiliza tecnología RAG (Retrieval Augmented Generation) combinando:\n",
        "                </p>\n",
        "                <ul style=\"list-style: none; color: #666; font-size: 14px;\">\n",
        "                    <li>🔹 Motor LLM: Ollama con Llama2</li>\n",
        "                    <li>🔹 Embeddings: multilingual-e5-small</li>\n",
        "                    <li>🔹 Base de datos vectorial: FAISS</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div style=\"border-top: 1px solid #ddd; padding-top: 15px;\">\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Creado para el curso de Inteligencia Artificial Aplicada - Universidad de los Andes<br>\n",
        "                    Por <a href=\"https://www.linkedin.com/in/camilovegabarbosa/\"\n",
        "                    target=\"_blank\" style=\"color: #2196F3; text-decoration: none;\">Camilo Vega</a>,\n",
        "                    Profesor de IA 🤖\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # --------- FUNCIONES DE CONTROL DE LA INTERFAZ ---------\n",
        "    # Función para limpiar el contexto y reiniciar\n",
        "    def clear_context():\n",
        "        # Eliminar la base de conocimiento y reiniciar el registro de archivos\n",
        "        rag_system.vector_store = None\n",
        "        rag_system.processed_files.clear()\n",
        "        return None\n",
        "\n",
        "    # Conectar eventos de la interfaz con funciones\n",
        "    message.submit(process_response, [message, chatbot, files, use_direct_ollama], [chatbot])\n",
        "    clear.click(clear_context, None, chatbot)\n",
        "\n",
        "# Lanzar la interfaz web\n",
        "print(\"🚀 Lanzando interfaz Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "5Yk--0McTuJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "85f33b6c-d1c6-4437-bc45-5af4f37ae0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Creando interfaz Gradio...\n",
            "🚀 Lanzando interfaz Gradio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-43c02599d002>:42: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "<ipython-input-27-43c02599d002>:42: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://32964197637239b696.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://32964197637239b696.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}