{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurapabon03/Final-IA-/blob/main/Copy_of_Final_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parcial Final Inteligencia Artificial\n",
        "\n",
        "Ángela Sofia Torres, Juan David Saldaña Rivera, Laura Pabón, Dario Montoya, Lucas Alvarado"
      ],
      "metadata": {
        "id": "26NxRaHnuLYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementación de RAG (Retrieval Augmented Generation) con Ollama para revisión de documentos económicos. Este script implementa un sistema de Generación Aumentada por Recuperación que añade información sobre documentos económicos para consultas sobre documentos usando Ollama para inferencia rápida.\n",
        "\n"
      ],
      "metadata": {
        "id": "IDd244CXGsc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 1: INSTALACIÓN DE DEPENDENCIAS\n",
        "# Estas líneas instalan las bibliotecas necesarias para el funcionamiento del sistema\n",
        "# Se ejecutan solamente en entornos como Google Colab o Jupyter Notebooks\n",
        "\n",
        "# Instalación de bibliotecas esenciales para RAG\n",
        "!pip install -q langchain langchain_community sentence-transformers pypdf python-docx docx2txt unstructured faiss-cpu gradio\n",
        "!pip install -q chromadb requests\n",
        "!pip install -q pandas\n",
        "!pip install -q pymupdf requests"
      ],
      "metadata": {
        "id": "IX_VoOPvKg9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 2: CONFIGURACIÓN DE OLLAMA\n",
        "# Ollama es una herramienta que permite ejecutar modelos de lenguaje localmente con menor uso de recursos que otros sistemas\n",
        "\n",
        "# Instalación de Ollama\n",
        "print(\"Instalando Ollama para inferencia rápida...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Iniciar el servicio de Ollama en segundo plano\n",
        "print(\"\\nIniciando servidor Ollama...\")\n",
        "!pkill ollama || true  # Detener cualquier instancia anterior\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar en segundo plano\n",
        "\n",
        "# Dar tiempo al servidor para inicializar\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama esté listo...\")\n",
        "time.sleep(15)  # Esperar 15 segundos\n",
        "\n",
        "# Verificar que el servidor esté activo\n",
        "print(\"\\nVerificando que el servidor Ollama esté respondiendo...\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no está respondiendo\"\n",
        "\n",
        "# Descargar el modelo LLM que usaremos (Llama2)\n",
        "print(\"\\nDescargando modelo llama2 desde Ollama...\")\n",
        "!ollama pull llama2"
      ],
      "metadata": {
        "id": "nyJWf9CNKpA4",
        "outputId": "e3c5a582-123b-4b36-a4a4-2ced2501f271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Ollama para inferencia rápida...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "Iniciando servidor Ollama...\n",
            "Esperando a que el servidor Ollama esté listo...\n",
            "\n",
            "Verificando que el servidor Ollama esté respondiendo...\n",
            "{\"models\":[{\"name\":\"llama2:latest\",\"model\":\"llama2:latest\",\"modified_at\":\"2025-05-16T02:51:17.825653216Z\",\"size\":3826793677,\"digest\":\"78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"7B\",\"quantization_level\":\"Q4_0\"}}]}\n",
            "Descargando modelo llama2 desde Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SECCIÓN 3: IMPORTACIÓN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "from langchain_community.llms import Ollama  # Integración LangChain-Ollama\n"
      ],
      "metadata": {
        "id": "lTOu6XV4MqlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 4: CONFIGURACIÓN BÁSICA\n",
        "\n",
        "# Configuración de logs para monitoreo y depuración\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constantes para configuración del sistema\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".csv\", \".txt\"]  # Formatos soportados\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"  # Modelo para codificación semántica\n",
        "OLLAMA_MODEL = \"llama2\"  # Modelo LLM local\n"
      ],
      "metadata": {
        "id": "NgJrarcKOpDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 5: CLASE PARA CARGA DE DOCUMENTOS\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Cargador unificado de documentos que soporta múltiples formatos.\n",
        "    Esta clase selecciona el cargador adecuado según la extensión del archivo.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> List:\n",
        "        \"\"\"\n",
        "        Carga un archivo basado en su extensión y devuelve los documentos procesados.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo a cargar\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos procesados con sus metadatos\n",
        "        \"\"\"\n",
        "        print(f\"Cargando archivo: {file_path}\")\n",
        "        ext = os.path.splitext(file_path)[1].lower()  # Obtener extensión del archivo\n",
        "\n",
        "        try:\n",
        "            # Seleccionar el cargador apropiado según el tipo de archivo\n",
        "            if ext == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)  # Para archivos PDF\n",
        "            elif ext in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)  # Para documentos Word\n",
        "            elif ext == '.csv':\n",
        "                loader = CSVLoader(file_path)  # Para archivos CSV\n",
        "            else:  # Para txt y otros formatos de texto\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "            # Ejecutar la carga del documento\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Enriquecer con metadatos para mejorar la recuperación y visualización\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    'title': os.path.basename(file_path),  # Nombre del archivo\n",
        "                    'type': 'document',  # Tipo de contenido\n",
        "                    'format': ext[1:],  # Formato sin el punto inicial\n",
        "                    'language': 'auto'  # Idioma (auto-detectado)\n",
        "                })\n",
        "\n",
        "            print(f\"✅ Archivo cargado exitosamente: {file_path}\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error al cargar {file_path}: {str(e)}\")\n",
        "            raise  # Re-lanzar la excepción para manejo superior"
      ],
      "metadata": {
        "id": "S0IV8BtROq-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 6: CLASE PRINCIPAL DEL SISTEMA RAG\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Sistema RAG completo con Ollama para consulta de documentos.\n",
        "\n",
        "    Esta clase implementa todo el flujo de trabajo RAG:\n",
        "    1. Carga y procesamiento de documentos\n",
        "    2. Generación de embeddings y almacenamiento vectorial\n",
        "    3. Recuperación de contexto relevante\n",
        "    4. Generación de respuestas mediante LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL, ollama_model: str = OLLAMA_MODEL):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema RAG con los modelos especificados.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Modelo para generar embeddings (representaciones vectoriales)\n",
        "            ollama_model: Modelo de lenguaje a utilizar con Ollama\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.ollama_model = ollama_model\n",
        "        self.embeddings = None  # Se inicializará posteriormente\n",
        "        self.vector_store = None  # Base de datos vectorial\n",
        "        self.qa_chain = None  # Cadena de pregunta-respuesta\n",
        "        self.is_initialized = False  # Flag de inicialización\n",
        "        self.processed_files = set()  # Conjunto para evitar procesar archivos duplicados\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"\n",
        "        Inicializa los componentes del sistema RAG:\n",
        "        - Modelo de embeddings\n",
        "        - Conexión con Ollama\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"🚀 Inicializando sistema RAG con Ollama...\")\n",
        "\n",
        "            # Inicializar el modelo de embeddings (usando CPU o GPU si está disponible)\n",
        "            print(\"📊 Cargando modelo de embeddings...\")\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=self.embedding_model,\n",
        "                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "                encode_kwargs={'normalize_embeddings': True}  # Normalización para mejor búsqueda\n",
        "            )\n",
        "\n",
        "            # Verificación de salud de Ollama - reintento si no responde\n",
        "            try:\n",
        "                response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "                if response.status_code != 200:\n",
        "                    print(\"⚠️ Advertencia: Ollama no está respondiendo correctamente. Reintentando inicialización...\")\n",
        "                    time.sleep(5)\n",
        "                    # Reinicio de emergencia del servicio Ollama\n",
        "                    subprocess.run(\"pkill ollama || true\", shell=True)\n",
        "                    subprocess.run(\"nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &\", shell=True)\n",
        "                    time.sleep(15)  # Esperar a que reinicie\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Advertencia al verificar Ollama: {str(e)}\")\n",
        "\n",
        "            # Configurar Ollama como modelo de lenguaje mediante LangChain\n",
        "            print(\"🧠 Configurando Ollama como LLM...\")\n",
        "            self.llm = Ollama(\n",
        "                model=self.ollama_model,\n",
        "                temperature=0.1,  # Temperatura baja para respuestas más deterministas\n",
        "                num_predict=512  # Máximo de tokens a generar\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True  # Marcar como inicializado\n",
        "            print(\"✅ Sistema RAG inicializado correctamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error durante la inicialización: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_documents(self, files: List[tempfile._TemporaryFileWrapper]) -> None:\n",
        "        \"\"\"\n",
        "        Procesa documentos cargados y actualiza la base de datos vectorial.\n",
        "\n",
        "        Args:\n",
        "            files: Lista de archivos temporales cargados por el usuario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            documents = []  # Lista para almacenar todos los documentos\n",
        "            new_files = []  # Seguimiento de archivos nuevos procesados\n",
        "\n",
        "            print(f\"📄 Procesando {len(files)} documento(s)...\")\n",
        "\n",
        "            # Filtrar y procesar solo archivos que no se han procesado antes\n",
        "            for file in files:\n",
        "                if file.name not in self.processed_files:\n",
        "                    docs = DocumentLoader.load_file(file.name)  # Cargar el archivo\n",
        "                    documents.extend(docs)  # Añadir documentos a la lista\n",
        "                    new_files.append(file.name)  # Registrar como nuevo\n",
        "                    self.processed_files.add(file.name)  # Marcar como procesado\n",
        "\n",
        "            # Si no hay archivos nuevos, terminar\n",
        "            if not new_files:\n",
        "                print(\"ℹ️ No hay documentos nuevos para procesar\")\n",
        "                return\n",
        "\n",
        "            # Verificar que se hayan cargado documentos\n",
        "            if not documents:\n",
        "                raise ValueError(\"No se pudieron cargar documentos.\")\n",
        "\n",
        "            # --------- DIVISIÓN DE DOCUMENTOS ---------\n",
        "            # Dividir documentos en fragmentos más pequeños para procesamiento eficiente\n",
        "            print(\"✂️ Dividiendo documentos en fragmentos...\")\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=800,  # Tamaño objetivo de cada fragmento (en caracteres)\n",
        "                chunk_overlap=200,  # Superposición entre fragmentos para mantener contexto\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separación\n",
        "                length_function=len  # Función para medir longitud\n",
        "            )\n",
        "\n",
        "            # Aplicar la división a todos los documentos\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            print(f\"🧩 Documentos divididos en {len(chunks)} fragmentos\")\n",
        "\n",
        "            # --------- VECTORIZACIÓN Y ALMACENAMIENTO ---------\n",
        "            # Crear o actualizar la base de datos vectorial con los nuevos fragmentos\n",
        "            print(\"🔍 Vectorizando fragmentos...\")\n",
        "            if self.vector_store is None:\n",
        "                # Primera carga: crear nueva base de datos vectorial\n",
        "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "            else:\n",
        "                # Carga adicional: añadir a la base de datos existente\n",
        "                self.vector_store.add_documents(chunks)\n",
        "\n",
        "            # --------- CONFIGURACIÓN DE PROMPT ---------\n",
        "            # Definir la plantilla de prompt para el LLM\n",
        "            prompt_template = \"\"\"\n",
        "            Contexto: {context}\n",
        "\n",
        "            Asume que eres un experto en economía. Basándote únicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa, con un lenguaje y tono académico y formal\n",
        "            Si la información no está en el contexto, indícalo explícitamente.\n",
        "\n",
        "            Pregunta: {question}\n",
        "            \"\"\"\n",
        "\n",
        "            # Crear objeto de prompt con variables\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template,\n",
        "                input_variables=[\"context\", \"question\"]  # Variables a rellenar\n",
        "            )\n",
        "\n",
        "            # --------- CONFIGURACIÓN DE CADENA QA ---------\n",
        "            # Inicializar la cadena de pregunta-respuesta con Ollama\n",
        "            print(\"⚙️ Configurando cadena de pregunta-respuesta con Ollama...\")\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,  # Modelo de lenguaje\n",
        "                chain_type=\"stuff\",  # Tipo de cadena (insertar todo el contexto de una vez)\n",
        "                retriever=self.vector_store.as_retriever(\n",
        "                    search_kwargs={\"k\": 6}  # Recuperar los 6 fragmentos más relevantes\n",
        "                ),\n",
        "                return_source_documents=True,  # Devolver documentos fuente para citas\n",
        "                chain_type_kwargs={\"prompt\": PROMPT}  # Usar nuestro prompt personalizado\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Procesamiento completado: {len(documents)} documentos añadidos a la base de conocimiento\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error procesando documentos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # MÉTODO 1: GENERACIÓN MEDIANTE LANGCHAIN (más robusto)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_response(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Genera una respuesta utilizando el framework LangChain.\n",
        "        Este método es más robusto y estructurado, con mejor manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la respuesta y fuentes utilizadas\n",
        "        \"\"\"\n",
        "        # Verificar que el sistema esté inicializado\n",
        "        if not self.is_initialized or self.vector_store is None:\n",
        "            return {\n",
        "                'answer': \"Por favor, carga algunos documentos antes de hacer preguntas.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"❓ Procesando pregunta: {question}\")\n",
        "\n",
        "            # Ejecutar la cadena QA con LangChain y Ollama\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Preparar la respuesta estructurada\n",
        "            response = {\n",
        "                'answer': result['result'],  # Respuesta generada\n",
        "                'sources': []  # Lista para fuentes\n",
        "            }\n",
        "\n",
        "            # Añadir información sobre las fuentes utilizadas\n",
        "            for doc in result['source_documents']:\n",
        "                source = {\n",
        "                    'title': doc.metadata.get('title', 'Desconocido'),\n",
        "                    'content': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    'metadata': doc.metadata\n",
        "                }\n",
        "                response['sources'].append(source)\n",
        "\n",
        "            print(\"✅ Respuesta generada con éxito usando el método LangChain\")\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error generando respuesta con LangChain: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # MÉTODO 2: GENERACIÓN DIRECTA CON API DE OLLAMA (más rápido)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_with_raw_ollama(self, question: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando directamente la API HTTP de Ollama.\n",
        "        Este método es más rápido pero menos robusto que el método LangChain.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "            context: Contexto recuperado de la base de conocimiento\n",
        "\n",
        "        Returns:\n",
        "            Texto de respuesta generado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Formatear el prompt con contexto y pregunta\n",
        "            formatted_prompt = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Asume que eres un experto en economía. Basándote únicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa, con un lenguaje académico y formal. Si la información no está en el contexto, indícalo explícitamente.\n",
        "\n",
        "Pregunta: {question}\n",
        "\"\"\"\n",
        "\n",
        "            # Configurar la llamada HTTP a Ollama\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            payload = {\n",
        "                \"model\": self.ollama_model,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"stream\": False,  # No usar streaming para simplificar\n",
        "                \"temperature\": 0.1,  # Consistente con el otro método\n",
        "                \"num_predict\": 512  # Número máximo de tokens\n",
        "            }\n",
        "\n",
        "            # Realizar la llamada API HTTP directa\n",
        "            print(\"Llamando a la API de Ollama con requests...\")\n",
        "            response = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            # Procesar la respuesta\n",
        "            if response.status_code == 200:\n",
        "                respuesta_json = response.json()\n",
        "                respuesta = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "                return respuesta\n",
        "            else:\n",
        "                return f\"Error en la API de Ollama: Código {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error llamando directamente a Ollama: {str(e)}\")\n",
        "            # Si falla, devolver mensaje de error y sugerir usar el método estándar\n",
        "            return \"Error al usar Ollama directamente. Intenta desactivar 'Usar Ollama directo'.\""
      ],
      "metadata": {
        "id": "_vM2X8Q3Pljr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 7: FUNCIÓN DE PROCESAMIENTO DE RESPUESTAS\n",
        "\n",
        "def process_response(user_input: str, chat_history, files, use_direct_ollama=True):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y genera una respuesta utilizando el sistema RAG.\n",
        "    Esta función coordina todo el proceso de consulta desde la entrada hasta la respuesta.\n",
        "\n",
        "    Args:\n",
        "        user_input: Pregunta o instrucción del usuario\n",
        "        chat_history: Historial de chat actual\n",
        "        files: Archivos cargados por el usuario\n",
        "        use_direct_ollama: Si es True, usa la API directa de Ollama; si es False, usa LangChain\n",
        "\n",
        "    Returns:\n",
        "        Historial de chat actualizado con la nueva pregunta y respuesta\n",
        "    \"\"\"\n",
        "    # Ignorar entradas vacías\n",
        "    if not user_input.strip():\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # PASO 1: Inicialización si es necesario\n",
        "        if not rag_system.is_initialized:\n",
        "            rag_system.initialize_system()\n",
        "\n",
        "        # PASO 2: Procesar documentos si hay archivos nuevos\n",
        "        if files:\n",
        "            rag_system.process_documents(files)\n",
        "\n",
        "        # Verificar que haya documentos procesados\n",
        "        if rag_system.vector_store is None:\n",
        "            answer = \"Por favor, carga algunos documentos antes de hacer preguntas.\"\n",
        "            chat_history.append((user_input, answer))\n",
        "            return chat_history\n",
        "\n",
        "        # PASO 3: Recuperar documentos relevantes para la consulta\n",
        "        print(\"🔍 Buscando documentos relevantes...\")\n",
        "        documents = rag_system.vector_store.similarity_search(user_input, k=6)\n",
        "        # Unir el contenido de los documentos como contexto\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "        # PASO 4: Generar respuesta según el método seleccionado\n",
        "        if use_direct_ollama:\n",
        "            # --------- MÉTODO DIRECTO (MÁS RÁPIDO) ---------\n",
        "            try:\n",
        "                print(\"🚀 Usando método directo de Ollama...\")\n",
        "                answer = rag_system.generate_with_raw_ollama(user_input, context)\n",
        "\n",
        "                # Implementación de fallback: si hay error, usar método estándar\n",
        "                if answer.startswith(\"Error\"):\n",
        "                    print(\"⚠️ Retrocediendo al método estándar...\")\n",
        "                    response = rag_system.generate_response(user_input)\n",
        "                    answer = response['answer']\n",
        "\n",
        "                    # Añadir información de fuentes\n",
        "                    sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                    if sources:\n",
        "                        answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "            except Exception as ollama_error:\n",
        "                # Manejo de error: si falla el método directo, usar el estándar\n",
        "                print(f\"❌ Error en método directo: {str(ollama_error)}\")\n",
        "                print(\"⚠️ Retrocediendo al método estándar...\")\n",
        "                response = rag_system.generate_response(user_input)\n",
        "                answer = response['answer']\n",
        "\n",
        "                # Añadir información de fuentes\n",
        "                sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                if sources:\n",
        "                    answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "        else:\n",
        "            # --------- MÉTODO ESTÁNDAR (MÁS ROBUSTO) ---------\n",
        "            print(\"🔄 Usando método estándar de LangChain...\")\n",
        "            response = rag_system.generate_response(user_input)\n",
        "            answer = response['answer']\n",
        "\n",
        "            # Añadir información de fuentes\n",
        "            sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "            if sources:\n",
        "                answer += \"\\n\\n📚 Fuentes consultadas:\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
        "\n",
        "        # PASO 5: Actualizar el historial de chat y retornar\n",
        "        chat_history.append((user_input, answer))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Manejo de errores generales\n",
        "        error_message = f\"Lo siento, ocurrió un error: {str(e)}\"\n",
        "        print(f\"❌ Error general en process_response: {str(e)}\")\n",
        "        chat_history.append((user_input, error_message))\n",
        "        return chat_history\n",
        "\n"
      ],
      "metadata": {
        "id": "jg2NGjPNTAR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 8: INICIALIZACIÓN DEL SISTEMA\n",
        "\n",
        "# Crear la instancia del sistema RAG\n",
        "print(\"🔧 Inicializando sistema RAG con Ollama...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"✅ Sistema RAG creado correctamente\")"
      ],
      "metadata": {
        "id": "PZRWgtL0Tm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a39ac3b-e703-4f2b-ea5a-84d0f3e29afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Inicializando sistema RAG con Ollama...\n",
            "✅ Sistema RAG creado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCIÓN 9: INTERFAZ GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la interfaz web con Gradio\n",
        "print(\"🌐 Creando interfaz Gradio...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Encabezado\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #2d333a;\">📚 RAG Assistant con Ollama</h1>\n",
        "            <p style=\"color: #4a5568;\">\n",
        "                Asistente IA para análisis y consulta de documentos usando Ollama\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Sección de carga de archivos y configuración\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Selector de archivos\n",
        "            files = gr.Files(\n",
        "                label=\"Carga tus documentos\",\n",
        "                file_types=SUPPORTED_FORMATS,\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "\n",
        "            # Opción para seleccionar método de generación\n",
        "            use_direct_ollama = gr.Checkbox(\n",
        "                label=\"Usar Ollama directo (más rápido)\",\n",
        "                value=False,  # Falso por defecto para mayor estabilidad\n",
        "                info=\"Hace llamadas directas a la API de Ollama para respuestas más rápidas.\"\n",
        "            )\n",
        "\n",
        "            # Información sobre formatos soportados\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"font-size: 0.9em; color: #666; margin-top: 0.5em;\">\n",
        "                    Formatos soportados: PDF, DOCX, CSV, TXT\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Interfaz de chat\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "        scale=2\n",
        "    )\n",
        "\n",
        "    # Área de entrada de texto y botón de limpieza\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            placeholder=\"💭 Pregunta cualquier cosa sobre tus documentos...\",\n",
        "            show_label=False,\n",
        "            container=False,\n",
        "            scale=8,\n",
        "            autofocus=True\n",
        "        )\n",
        "        clear = gr.Button(\"🗑️ Limpiar\", size=\"sm\", scale=1)\n",
        "\n",
        "    # Sección de instrucciones\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
        "            <h3 style=\"color: #2d333a; margin-bottom: 10px;\">🔍 Cómo usar:</h3>\n",
        "            <ol style=\"color: #666; margin-left: 20px;\">\n",
        "                <li>Carga uno o más documentos (PDF, DOCX, CSV, o TXT)</li>\n",
        "                <li>Espera a que los documentos sean procesados</li>\n",
        "                <li>Haz preguntas sobre el contenido de tus documentos</li>\n",
        "                <li>Activa \"Usar Ollama directo\" para respuestas más rápidas (desactívalo si hay errores)</li>\n",
        "            </ol>\n",
        "            <p style=\"color: #666; font-style: italic; margin-top: 10px;\">\n",
        "                Nota: La primera respuesta puede tardar un poco. Desde la segunda respuesta es más rápido.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Pie de página con información técnica y créditos\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 20px auto; padding: 20px;\n",
        "                    background-color: #f8f9fa; border-radius: 10px;\">\n",
        "            <div style=\"margin-bottom: 15px;\">\n",
        "                <h3 style=\"color: #2d333a;\">⚡ Sobre este asistente</h3>\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Esta aplicación utiliza tecnología RAG (Retrieval Augmented Generation) combinando:\n",
        "                </p>\n",
        "                <ul style=\"list-style: none; color: #666; font-size: 14px;\">\n",
        "                    <li>🔹 Motor LLM: Ollama con Llama2</li>\n",
        "                    <li>🔹 Embeddings: multilingual-e5-small</li>\n",
        "                    <li>🔹 Base de datos vectorial: FAISS</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div style=\"border-top: 1px solid #ddd; padding-top: 15px;\">\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Proyecto examen final curso Inteligencia Artificial Aplicada a la Economía - Universidad de los Andes<br>\n",
        "\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # --------- FUNCIONES DE CONTROL DE LA INTERFAZ ---------\n",
        "    # Función para limpiar el contexto y reiniciar\n",
        "    def clear_context():\n",
        "        # Eliminar la base de conocimiento y reiniciar el registro de archivos\n",
        "        rag_system.vector_store = None\n",
        "        rag_system.processed_files.clear()\n",
        "        return None\n",
        "\n",
        "    # Conectar eventos de la interfaz con funciones\n",
        "    message.submit(process_response, [message, chatbot, files, use_direct_ollama], [chatbot])\n",
        "    clear.click(clear_context, None, chatbot)\n",
        "\n",
        "# Lanzar la interfaz web\n",
        "print(\"🚀 Lanzando interfaz Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "5Yk--0McTuJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db833182-c529-4ce4-ae8d-d3e3b50ee532"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌐 Creando interfaz Gradio...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-50-43c02599d002>:42: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "<ipython-input-50-43c02599d002>:42: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Lanzando interfaz Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b99edcfb7b341d7112.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b99edcfb7b341d7112.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/d9657f5034c853c4a36bd7fe10238cc76f0b6285d38ddff2710ca39d70259130/3.2 Nunn - The Importance of History for Economic Development.docx\n",
            "✅ Archivo cargado exitosamente: /tmp/gradio/d9657f5034c853c4a36bd7fe10238cc76f0b6285d38ddff2710ca39d70259130/3.2 Nunn - The Importance of History for Economic Development.docx\n",
            "✂️ Dividiendo documentos en fragmentos...\n",
            "🧩 Documentos divididos en 216 fragmentos\n",
            "🔍 Vectorizando fragmentos...\n",
            "⚙️ Configurando cadena de pregunta-respuesta con Ollama...\n",
            "✅ Procesamiento completado: 1 documentos añadidos a la base de conocimiento\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: De qué se trata el texto?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n",
            "📄 Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "✅ Archivo cargado exitosamente: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "✂️ Dividiendo documentos en fragmentos...\n",
            "🧩 Documentos divididos en 214 fragmentos\n",
            "🔍 Vectorizando fragmentos...\n",
            "⚙️ Configurando cadena de pregunta-respuesta con Ollama...\n",
            "✅ Procesamiento completado: 61 documentos añadidos a la base de conocimiento\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: Cuál es la conclusión del texto?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n",
            "📄 Procesando 1 documento(s)...\n",
            "Cargando archivo: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "✅ Archivo cargado exitosamente: /tmp/gradio/c251f4c8904c57ecd508e9f45604c89375aeb0ef6f1e7de69ee01f99d8cdeb04/Paper Abhijit Banerjee.pdf\n",
            "✂️ Dividiendo documentos en fragmentos...\n",
            "🧩 Documentos divididos en 214 fragmentos\n",
            "🔍 Vectorizando fragmentos...\n",
            "⚙️ Configurando cadena de pregunta-respuesta con Ollama...\n",
            "✅ Procesamiento completado: 61 documentos añadidos a la base de conocimiento\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: Cuáles son los resultados de los experimentos mencionados en el artículo?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n",
            "📄 Procesando 1 documento(s)...\n",
            "ℹ️ No hay documentos nuevos para procesar\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: Quiénes son los autores del texto?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n",
            "📄 Procesando 1 documento(s)...\n",
            "ℹ️ No hay documentos nuevos para procesar\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: Esos experimentos son replicables en otros países en desarrollo?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n",
            "📄 Procesando 1 documento(s)...\n",
            "ℹ️ No hay documentos nuevos para procesar\n",
            "🔍 Buscando documentos relevantes...\n",
            "🔄 Usando método estándar de LangChain...\n",
            "❓ Procesando pregunta: Cuáles son las conclusiones del texto?\n",
            "✅ Respuesta generada con éxito usando el método LangChain\n"
          ]
        }
      ]
    }
  ]
}