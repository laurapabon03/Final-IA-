{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurapabon03/Final-IA-/blob/main/Copy_of_Final_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parcial Final Inteligencia Artificial\n",
        "\n",
        "√Ångela Sofia Torres, Juan David Salda√±a Rivera, Laura Pab√≥n, Dario Montoya, Lucas Alvarado"
      ],
      "metadata": {
        "id": "26NxRaHnuLYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 1: INSTALACI√ìN DE DEPENDENCIAS\n",
        "# Estas l√≠neas instalan las bibliotecas necesarias para el funcionamiento del sistema\n",
        "# Se ejecutan solamente en entornos como Google Colab o Jupyter Notebooks\n",
        "\n",
        "# Instalaci√≥n de bibliotecas esenciales para RAG\n",
        "!pip install -q langchain langchain_community sentence-transformers pypdf python-docx docx2txt unstructured faiss-cpu gradio\n",
        "!pip install -q chromadb requests"
      ],
      "metadata": {
        "id": "IX_VoOPvKg9d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 2: CONFIGURACI√ìN DE OLLAMA\n",
        "# Ollama es una herramienta que permite ejecutar modelos de lenguaje localmente con menor uso de recursos que otros sistemas\n",
        "\n",
        "# Instalaci√≥n de Ollama\n",
        "print(\"Instalando Ollama para inferencia r√°pida...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Iniciar el servicio de Ollama en segundo plano\n",
        "print(\"\\nIniciando servidor Ollama...\")\n",
        "!pkill ollama || true  # Detener cualquier instancia anterior\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar en segundo plano\n",
        "\n",
        "# Dar tiempo al servidor para inicializar\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama est√© listo...\")\n",
        "time.sleep(15)  # Esperar 15 segundos\n",
        "\n",
        "# Verificar que el servidor est√© activo\n",
        "print(\"\\nVerificando que el servidor Ollama est√© respondiendo...\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no est√° respondiendo\"\n",
        "\n",
        "# Descargar el modelo LLM que usaremos (Llama2)\n",
        "print(\"\\nDescargando modelo llama2 desde Ollama...\")\n",
        "!ollama pull llama2"
      ],
      "metadata": {
        "id": "nyJWf9CNKpA4",
        "outputId": "cfb1ff30-8562-4777-e5c2-7c19a7b9eb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Ollama para inferencia r√°pida...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "Iniciando servidor Ollama...\n",
            "Esperando a que el servidor Ollama est√© listo...\n",
            "\n",
            "Verificando que el servidor Ollama est√© respondiendo...\n",
            "{\"models\":[{\"name\":\"llama2:latest\",\"model\":\"llama2:latest\",\"modified_at\":\"2025-05-15T20:54:49.744415211Z\",\"size\":3826793677,\"digest\":\"78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"7B\",\"quantization_level\":\"Q4_0\"}}]}\n",
            "Descargando modelo llama2 desde Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SECCI√ìN 3: IMPORTACI√ìN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "#SECCI√ìN 3: IMPORTACI√ìN DE BIBLIOTECAS\n",
        "# Estas bibliotecas proporcionan las funcionalidades esenciales para el sistema RAG\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "import subprocess\n",
        "import json\n",
        "import requests  # Para llamadas HTTP directas a Ollama\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Componentes de LangChain para RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Para dividir documentos\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Para crear embeddings\n",
        "from langchain.vectorstores import FAISS  # Base de datos vectorial\n",
        "from langchain.chains import RetrievalQA  # Framework para consultas RAG\n",
        "from langchain.prompts import PromptTemplate  # Para definir prompts\n",
        "from langchain_community.document_loaders import (  # Cargadores de documentos\n",
        "    PyPDFLoader,  # Para PDF\n",
        "    Docx2txtLoader,  # Para DOCX\n",
        "    CSVLoader,  # Para CSV\n",
        "    UnstructuredFileLoader  # Para texto plano y otros formatos\n",
        ")\n",
        "from langchain_community.llms import Ollama  # Integraci√≥n LangChain-Ollama\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "lTOu6XV4MqlJ",
        "outputId": "b938150a-5920-4fda-9aa3-5a636b322c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Aqu√≠ seleccionas tu archivo kaggle.json\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Crear la carpeta .kaggle\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Mover kaggle.json all√≠\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "\n",
        "# Cambiar permisos\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "!kaggle datasets download -d nitindatta/finance-data\n",
        "\n",
        "!unzip -o -q finance-data.zip -d finance-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "XJM7cBWDKQMw",
        "outputId": "1868c6b9-26be-42e2-a059-0a5bbb602b68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5cec8cb-de4d-4f0d-96a7-00b819b1a035\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f5cec8cb-de4d-4f0d-96a7-00b819b1a035\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/nitindatta/finance-data\n",
            "License(s): unknown\n",
            "finance-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1.1: Configurar kaggle.json (ya subido)\n",
        "import os\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Paso 2: Descargar el dataset\n",
        "!kaggle datasets download -d nitindatta/finance-data\n",
        "!unzip -o finance-data.zip -d finance-data\n",
        "\n",
        "# Paso 3: Cargar los datos\n",
        "import pandas as pd\n",
        "\n",
        "# Explorar archivos disponibles\n",
        "print(\"Archivos disponibles:\", os.listdir(\"finance-data\"))\n",
        "\n",
        "# Verificar columnas disponibles para escoger el CSV adecuado\n",
        "df = pd.read_csv(\"finance-data/Finance_data.csv\")  # Ajustado seg√∫n nombre real\n",
        "print(\"Columnas disponibles:\", df.columns.tolist())\n",
        "\n",
        "# (Si 'date' y 'headline' no existen, muestra los primeros registros)\n",
        "print(df.head(3))\n",
        "\n",
        "# Si existen columnas 'date' y 'headline', procesarlas\n",
        "if 'date' in df.columns and 'headline' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m')\n",
        "    df = df.dropna(subset=['date', 'headline'])\n",
        "    print(df[['date', 'headline']].head())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Las columnas 'date' y/o 'headline' no est√°n presentes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Eu8eDzy3Bpm",
        "outputId": "fcd595ba-b70e-488d-b9ec-681d097d353f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/nitindatta/finance-data\n",
            "License(s): unknown\n",
            "finance-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  finance-data.zip\n",
            "  inflating: finance-data/Finance_data.csv  \n",
            "  inflating: finance-data/Original_data.csv  \n",
            "Archivos disponibles: ['Finance_data.csv', 'Original_data.csv']\n",
            "Columnas disponibles: ['gender', 'age', 'Investment_Avenues', 'Mutual_Funds', 'Equity_Market', 'Debentures', 'Government_Bonds', 'Fixed_Deposits', 'PPF', 'Gold', 'Stock_Marktet', 'Factor', 'Objective', 'Purpose', 'Duration', 'Invest_Monitor', 'Expect', 'Avenue', 'What are your savings objectives?', 'Reason_Equity', 'Reason_Mutual', 'Reason_Bonds', 'Reason_FD', 'Source']\n",
            "   gender  age Investment_Avenues  Mutual_Funds  Equity_Market  Debentures  \\\n",
            "0  Female   34                Yes             1              2           5   \n",
            "1  Female   23                Yes             4              3           2   \n",
            "2    Male   30                Yes             3              6           4   \n",
            "\n",
            "   Government_Bonds  Fixed_Deposits  PPF  Gold  ...           Duration  \\\n",
            "0                 3               7    6     4  ...          1-3 years   \n",
            "1                 1               5    6     7  ...  More than 5 years   \n",
            "2                 2               5    1     7  ...          3-5 years   \n",
            "\n",
            "  Invest_Monitor   Expect       Avenue What are your savings objectives?  \\\n",
            "0        Monthly  20%-30%  Mutual Fund                   Retirement Plan   \n",
            "1         Weekly  20%-30%  Mutual Fund                       Health Care   \n",
            "2          Daily  20%-30%       Equity                   Retirement Plan   \n",
            "\n",
            "          Reason_Equity   Reason_Mutual     Reason_Bonds            Reason_FD  \\\n",
            "0  Capital Appreciation  Better Returns  Safe Investment        Fixed Returns   \n",
            "1              Dividend  Better Returns  Safe Investment  High Interest Rates   \n",
            "2  Capital Appreciation    Tax Benefits  Assured Returns        Fixed Returns   \n",
            "\n",
            "                     Source  \n",
            "0  Newspapers and Magazines  \n",
            "1     Financial Consultants  \n",
            "2                Television  \n",
            "\n",
            "[3 rows x 24 columns]\n",
            "‚ö†Ô∏è Las columnas 'date' y/o 'headline' no est√°n presentes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 4: CONFIGURACI√ìN B√ÅSICA\n",
        "\n",
        "# Configuraci√≥n de logs para monitoreo y depuraci√≥n\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constantes para configuraci√≥n del sistema\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".doc\", \".csv\", \".txt\"]  # Formatos soportados\n",
        "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"  # Modelo para codificaci√≥n sem√°ntica\n",
        "OLLAMA_MODEL = \"llama2\"  # Modelo LLM local\n"
      ],
      "metadata": {
        "id": "NgJrarcKOpDa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 5: CLASE PARA CARGA DE DOCUMENTOS\n",
        "\n",
        "class DocumentLoader:\n",
        "    \"\"\"\n",
        "    Cargador unificado de documentos que soporta m√∫ltiples formatos.\n",
        "    Esta clase selecciona el cargador adecuado seg√∫n la extensi√≥n del archivo.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_file(file_path: str) -> List:\n",
        "        \"\"\"\n",
        "        Carga un archivo basado en su extensi√≥n y devuelve los documentos procesados.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo a cargar\n",
        "\n",
        "        Returns:\n",
        "            Lista de documentos procesados con sus metadatos\n",
        "        \"\"\"\n",
        "        print(f\"Cargando archivo: {file_path}\")\n",
        "        ext = os.path.splitext(file_path)[1].lower()  # Obtener extensi√≥n del archivo\n",
        "\n",
        "        try:\n",
        "            # Seleccionar el cargador apropiado seg√∫n el tipo de archivo\n",
        "            if ext == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)  # Para archivos PDF\n",
        "            elif ext in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)  # Para documentos Word\n",
        "            elif ext == '.csv':\n",
        "                loader = CSVLoader(file_path)  # Para archivos CSV\n",
        "            else:  # Para txt y otros formatos de texto\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "            # Ejecutar la carga del documento\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Enriquecer con metadatos para mejorar la recuperaci√≥n y visualizaci√≥n\n",
        "            for doc in documents:\n",
        "                doc.metadata.update({\n",
        "                    'title': os.path.basename(file_path),  # Nombre del archivo\n",
        "                    'type': 'document',  # Tipo de contenido\n",
        "                    'format': ext[1:],  # Formato sin el punto inicial\n",
        "                    'language': 'auto'  # Idioma (auto-detectado)\n",
        "                })\n",
        "\n",
        "            print(f\"‚úÖ Archivo cargado exitosamente: {file_path}\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al cargar {file_path}: {str(e)}\")\n",
        "            raise  # Re-lanzar la excepci√≥n para manejo superior"
      ],
      "metadata": {
        "id": "S0IV8BtROq-l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 6: CLASE PRINCIPAL DEL SISTEMA RAG\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Sistema RAG completo con Ollama para consulta de documentos.\n",
        "\n",
        "    Esta clase implementa todo el flujo de trabajo RAG:\n",
        "    1. Carga y procesamiento de documentos\n",
        "    2. Generaci√≥n de embeddings y almacenamiento vectorial\n",
        "    3. Recuperaci√≥n de contexto relevante\n",
        "    4. Generaci√≥n de respuestas mediante LLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model: str = EMBEDDING_MODEL, ollama_model: str = OLLAMA_MODEL):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema RAG con los modelos especificados.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Modelo para generar embeddings (representaciones vectoriales)\n",
        "            ollama_model: Modelo de lenguaje a utilizar con Ollama\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.ollama_model = ollama_model\n",
        "        self.embeddings = None  # Se inicializar√° posteriormente\n",
        "        self.vector_store = None  # Base de datos vectorial\n",
        "        self.qa_chain = None  # Cadena de pregunta-respuesta\n",
        "        self.is_initialized = False  # Flag de inicializaci√≥n\n",
        "        self.processed_files = set()  # Conjunto para evitar procesar archivos duplicados\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"\n",
        "        Inicializa los componentes del sistema RAG:\n",
        "        - Modelo de embeddings\n",
        "        - Conexi√≥n con Ollama\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üöÄ Inicializando sistema RAG con Ollama...\")\n",
        "\n",
        "            # Inicializar el modelo de embeddings (usando CPU o GPU si est√° disponible)\n",
        "            print(\"üìä Cargando modelo de embeddings...\")\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=self.embedding_model,\n",
        "                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "                encode_kwargs={'normalize_embeddings': True}  # Normalizaci√≥n para mejor b√∫squeda\n",
        "            )\n",
        "\n",
        "            # Verificaci√≥n de salud de Ollama - reintento si no responde\n",
        "            try:\n",
        "                response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "                if response.status_code != 200:\n",
        "                    print(\"‚ö†Ô∏è Advertencia: Ollama no est√° respondiendo correctamente. Reintentando inicializaci√≥n...\")\n",
        "                    time.sleep(5)\n",
        "                    # Reinicio de emergencia del servicio Ollama\n",
        "                    subprocess.run(\"pkill ollama || true\", shell=True)\n",
        "                    subprocess.run(\"nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &\", shell=True)\n",
        "                    time.sleep(15)  # Esperar a que reinicie\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Advertencia al verificar Ollama: {str(e)}\")\n",
        "\n",
        "            # Configurar Ollama como modelo de lenguaje mediante LangChain\n",
        "            print(\"üß† Configurando Ollama como LLM...\")\n",
        "            self.llm = Ollama(\n",
        "                model=self.ollama_model,\n",
        "                temperature=0.1,  # Temperatura baja para respuestas m√°s deterministas\n",
        "                num_predict=512  # M√°ximo de tokens a generar\n",
        "            )\n",
        "\n",
        "            self.is_initialized = True  # Marcar como inicializado\n",
        "            print(\"‚úÖ Sistema RAG inicializado correctamente\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error durante la inicializaci√≥n: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process_documents(self, files: List[tempfile._TemporaryFileWrapper]) -> None:\n",
        "        \"\"\"\n",
        "        Procesa documentos cargados y actualiza la base de datos vectorial.\n",
        "\n",
        "        Args:\n",
        "            files: Lista de archivos temporales cargados por el usuario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            documents = []  # Lista para almacenar todos los documentos\n",
        "            new_files = []  # Seguimiento de archivos nuevos procesados\n",
        "\n",
        "            print(f\"üìÑ Procesando {len(files)} documento(s)...\")\n",
        "\n",
        "            # Filtrar y procesar solo archivos que no se han procesado antes\n",
        "            for file in files:\n",
        "                if file.name not in self.processed_files:\n",
        "                    docs = DocumentLoader.load_file(file.name)  # Cargar el archivo\n",
        "                    documents.extend(docs)  # A√±adir documentos a la lista\n",
        "                    new_files.append(file.name)  # Registrar como nuevo\n",
        "                    self.processed_files.add(file.name)  # Marcar como procesado\n",
        "\n",
        "            # Si no hay archivos nuevos, terminar\n",
        "            if not new_files:\n",
        "                print(\"‚ÑπÔ∏è No hay documentos nuevos para procesar\")\n",
        "                return\n",
        "\n",
        "            # Verificar que se hayan cargado documentos\n",
        "            if not documents:\n",
        "                raise ValueError(\"No se pudieron cargar documentos.\")\n",
        "\n",
        "            # --------- DIVISI√ìN DE DOCUMENTOS ---------\n",
        "            # Dividir documentos en fragmentos m√°s peque√±os para procesamiento eficiente\n",
        "            print(\"‚úÇÔ∏è Dividiendo documentos en fragmentos...\")\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=800,  # Tama√±o objetivo de cada fragmento (en caracteres)\n",
        "                chunk_overlap=200,  # Superposici√≥n entre fragmentos para mantener contexto\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separaci√≥n\n",
        "                length_function=len  # Funci√≥n para medir longitud\n",
        "            )\n",
        "\n",
        "            # Aplicar la divisi√≥n a todos los documentos\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            print(f\"üß© Documentos divididos en {len(chunks)} fragmentos\")\n",
        "\n",
        "            # --------- VECTORIZACI√ìN Y ALMACENAMIENTO ---------\n",
        "            # Crear o actualizar la base de datos vectorial con los nuevos fragmentos\n",
        "            print(\"üîç Vectorizando fragmentos...\")\n",
        "            if self.vector_store is None:\n",
        "                # Primera carga: crear nueva base de datos vectorial\n",
        "                self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
        "            else:\n",
        "                # Carga adicional: a√±adir a la base de datos existente\n",
        "                self.vector_store.add_documents(chunks)\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE PROMPT ---------\n",
        "            # Definir la plantilla de prompt para el LLM\n",
        "            prompt_template = \"\"\"\n",
        "            Contexto: {context}\n",
        "\n",
        "            Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "            Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "            Pregunta: {question}\n",
        "            \"\"\"\n",
        "\n",
        "            # Crear objeto de prompt con variables\n",
        "            PROMPT = PromptTemplate(\n",
        "                template=prompt_template,\n",
        "                input_variables=[\"context\", \"question\"]  # Variables a rellenar\n",
        "            )\n",
        "\n",
        "            # --------- CONFIGURACI√ìN DE CADENA QA ---------\n",
        "            # Inicializar la cadena de pregunta-respuesta con Ollama\n",
        "            print(\"‚öôÔ∏è Configurando cadena de pregunta-respuesta con Ollama...\")\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,  # Modelo de lenguaje\n",
        "                chain_type=\"stuff\",  # Tipo de cadena (insertar todo el contexto de una vez)\n",
        "                retriever=self.vector_store.as_retriever(\n",
        "                    search_kwargs={\"k\": 6}  # Recuperar los 6 fragmentos m√°s relevantes\n",
        "                ),\n",
        "                return_source_documents=True,  # Devolver documentos fuente para citas\n",
        "                chain_type_kwargs={\"prompt\": PROMPT}  # Usar nuestro prompt personalizado\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Procesamiento completado: {len(documents)} documentos a√±adidos a la base de conocimiento\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error procesando documentos: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 1: GENERACI√ìN MEDIANTE LANGCHAIN (m√°s robusto)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_response(self, question: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Genera una respuesta utilizando el framework LangChain.\n",
        "        Este m√©todo es m√°s robusto y estructurado, con mejor manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la respuesta y fuentes utilizadas\n",
        "        \"\"\"\n",
        "        # Verificar que el sistema est√© inicializado\n",
        "        if not self.is_initialized or self.vector_store is None:\n",
        "            return {\n",
        "                'answer': \"Por favor, carga algunos documentos antes de hacer preguntas.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"‚ùì Procesando pregunta: {question}\")\n",
        "\n",
        "            # Ejecutar la cadena QA con LangChain y Ollama\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Preparar la respuesta estructurada\n",
        "            response = {\n",
        "                'answer': result['result'],  # Respuesta generada\n",
        "                'sources': []  # Lista para fuentes\n",
        "            }\n",
        "\n",
        "            # A√±adir informaci√≥n sobre las fuentes utilizadas\n",
        "            for doc in result['source_documents']:\n",
        "                source = {\n",
        "                    'title': doc.metadata.get('title', 'Desconocido'),\n",
        "                    'content': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                    'metadata': doc.metadata\n",
        "                }\n",
        "                response['sources'].append(source)\n",
        "\n",
        "            print(\"‚úÖ Respuesta generada con √©xito usando el m√©todo LangChain\")\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generando respuesta con LangChain: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # ============================================================================\n",
        "    # M√âTODO 2: GENERACI√ìN DIRECTA CON API DE OLLAMA (m√°s r√°pido)\n",
        "    # ============================================================================\n",
        "\n",
        "    def generate_with_raw_ollama(self, question: str, context: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando directamente la API HTTP de Ollama.\n",
        "        Este m√©todo es m√°s r√°pido pero menos robusto que el m√©todo LangChain.\n",
        "\n",
        "        Args:\n",
        "            question: Pregunta del usuario\n",
        "            context: Contexto recuperado de la base de conocimiento\n",
        "\n",
        "        Returns:\n",
        "            Texto de respuesta generado\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Formatear el prompt con contexto y pregunta\n",
        "            formatted_prompt = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Bas√°ndote √∫nicamente en el contexto proporcionado, responde a la siguiente pregunta de manera clara y concisa.\n",
        "Si la informaci√≥n no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "\n",
        "Pregunta: {question}\n",
        "\"\"\"\n",
        "\n",
        "            # Configurar la llamada HTTP a Ollama\n",
        "            headers = {\"Content-Type\": \"application/json\"}\n",
        "            payload = {\n",
        "                \"model\": self.ollama_model,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"stream\": False,  # No usar streaming para simplificar\n",
        "                \"temperature\": 0.1,  # Consistente con el otro m√©todo\n",
        "                \"num_predict\": 512  # N√∫mero m√°ximo de tokens\n",
        "            }\n",
        "\n",
        "            # Realizar la llamada API HTTP directa\n",
        "            print(\"Llamando a la API de Ollama con requests...\")\n",
        "            response = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            # Procesar la respuesta\n",
        "            if response.status_code == 200:\n",
        "                respuesta_json = response.json()\n",
        "                respuesta = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "                return respuesta\n",
        "            else:\n",
        "                return f\"Error en la API de Ollama: C√≥digo {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error llamando directamente a Ollama: {str(e)}\")\n",
        "            # Si falla, devolver mensaje de error y sugerir usar el m√©todo est√°ndar\n",
        "            return \"Error al usar Ollama directamente. Intenta desactivar 'Usar Ollama directo'.\""
      ],
      "metadata": {
        "id": "_vM2X8Q3Pljr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 7: FUNCI√ìN DE PROCESAMIENTO DE RESPUESTAS\n",
        "\n",
        "def process_response(user_input: str, chat_history, files, use_direct_ollama=True):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y genera una respuesta utilizando el sistema RAG.\n",
        "    Esta funci√≥n coordina todo el proceso de consulta desde la entrada hasta la respuesta.\n",
        "\n",
        "    Args:\n",
        "        user_input: Pregunta o instrucci√≥n del usuario\n",
        "        chat_history: Historial de chat actual\n",
        "        files: Archivos cargados por el usuario\n",
        "        use_direct_ollama: Si es True, usa la API directa de Ollama; si es False, usa LangChain\n",
        "\n",
        "    Returns:\n",
        "        Historial de chat actualizado con la nueva pregunta y respuesta\n",
        "    \"\"\"\n",
        "    # Ignorar entradas vac√≠as\n",
        "    if not user_input.strip():\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # PASO 1: Inicializaci√≥n si es necesario\n",
        "        if not rag_system.is_initialized:\n",
        "            rag_system.initialize_system()\n",
        "\n",
        "        # PASO 2: Procesar documentos si hay archivos nuevos\n",
        "        if files:\n",
        "            rag_system.process_documents(files)\n",
        "\n",
        "        # Verificar que haya documentos procesados\n",
        "        if rag_system.vector_store is None:\n",
        "            answer = \"Por favor, carga algunos documentos antes de hacer preguntas.\"\n",
        "            chat_history.append((user_input, answer))\n",
        "            return chat_history\n",
        "\n",
        "        # PASO 3: Recuperar documentos relevantes para la consulta\n",
        "        print(\"üîç Buscando documentos relevantes...\")\n",
        "        documents = rag_system.vector_store.similarity_search(user_input, k=6)\n",
        "        # Unir el contenido de los documentos como contexto\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "        # PASO 4: Generar respuesta seg√∫n el m√©todo seleccionado\n",
        "        if use_direct_ollama:\n",
        "            # --------- M√âTODO DIRECTO (M√ÅS R√ÅPIDO) ---------\n",
        "            try:\n",
        "                print(\"üöÄ Usando m√©todo directo de Ollama...\")\n",
        "                answer = rag_system.generate_with_raw_ollama(user_input, context)\n",
        "\n",
        "                # Implementaci√≥n de fallback: si hay error, usar m√©todo est√°ndar\n",
        "                if answer.startswith(\"Error\"):\n",
        "                    print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                    response = rag_system.generate_response(user_input)\n",
        "                    answer = response['answer']\n",
        "\n",
        "                    # A√±adir informaci√≥n de fuentes\n",
        "                    sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                    if sources:\n",
        "                        answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "            except Exception as ollama_error:\n",
        "                # Manejo de error: si falla el m√©todo directo, usar el est√°ndar\n",
        "                print(f\"‚ùå Error en m√©todo directo: {str(ollama_error)}\")\n",
        "                print(\"‚ö†Ô∏è Retrocediendo al m√©todo est√°ndar...\")\n",
        "                response = rag_system.generate_response(user_input)\n",
        "                answer = response['answer']\n",
        "\n",
        "                # A√±adir informaci√≥n de fuentes\n",
        "                sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "                if sources:\n",
        "                    answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "        else:\n",
        "            # --------- M√âTODO EST√ÅNDAR (M√ÅS ROBUSTO) ---------\n",
        "            print(\"üîÑ Usando m√©todo est√°ndar de LangChain...\")\n",
        "            response = rag_system.generate_response(user_input)\n",
        "            answer = response['answer']\n",
        "\n",
        "            # A√±adir informaci√≥n de fuentes\n",
        "            sources = set([doc.metadata.get('title', 'Desconocido') for doc in documents[:3]])\n",
        "            if sources:\n",
        "                answer += \"\\n\\nüìö Fuentes consultadas:\\n\" + \"\\n\".join([f\"‚Ä¢ {source}\" for source in sources])\n",
        "\n",
        "        # PASO 5: Actualizar el historial de chat y retornar\n",
        "        chat_history.append((user_input, answer))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Manejo de errores generales\n",
        "        error_message = f\"Lo siento, ocurri√≥ un error: {str(e)}\"\n",
        "        print(f\"‚ùå Error general en process_response: {str(e)}\")\n",
        "        chat_history.append((user_input, error_message))\n",
        "        return chat_history\n",
        "\n"
      ],
      "metadata": {
        "id": "jg2NGjPNTAR0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 8: INICIALIZACI√ìN DEL SISTEMA\n",
        "\n",
        "# Crear la instancia del sistema RAG\n",
        "print(\"üîß Inicializando sistema RAG con Ollama...\")\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ Sistema RAG creado correctamente\")"
      ],
      "metadata": {
        "id": "PZRWgtL0Tm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0005dbb8-a01a-4c41-9df4-c0108cc509b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Inicializando sistema RAG con Ollama...\n",
            "‚úÖ Sistema RAG creado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SD5tNWHnVlIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECCI√ìN 9: INTERFAZ GRADIO\n",
        "# ============================================================================\n",
        "\n",
        "# Crear la interfaz web con Gradio\n",
        "print(\"üåê Creando interfaz Gradio...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # Encabezado\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #2d333a;\">üìö RAG Assistant con Ollama</h1>\n",
        "            <p style=\"color: #4a5568;\">\n",
        "                Asistente IA para an√°lisis y consulta de documentos usando Ollama\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Secci√≥n de carga de archivos y configuraci√≥n\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Selector de archivos\n",
        "            files = gr.Files(\n",
        "                label=\"Carga tus documentos\",\n",
        "                file_types=SUPPORTED_FORMATS,\n",
        "                file_count=\"multiple\"\n",
        "            )\n",
        "\n",
        "            # Opci√≥n para seleccionar m√©todo de generaci√≥n\n",
        "            use_direct_ollama = gr.Checkbox(\n",
        "                label=\"Usar Ollama directo (m√°s r√°pido)\",\n",
        "                value=False,  # Falso por defecto para mayor estabilidad\n",
        "                info=\"Hace llamadas directas a la API de Ollama para respuestas m√°s r√°pidas.\"\n",
        "            )\n",
        "\n",
        "            # Informaci√≥n sobre formatos soportados\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"font-size: 0.9em; color: #666; margin-top: 0.5em;\">\n",
        "                    Formatos soportados: PDF, DOCX, CSV, TXT\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    # Interfaz de chat\n",
        "    chatbot = gr.Chatbot(\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "        scale=2\n",
        "    )\n",
        "\n",
        "    # √Årea de entrada de texto y bot√≥n de limpieza\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            placeholder=\"üí≠ Pregunta cualquier cosa sobre tus documentos...\",\n",
        "            show_label=False,\n",
        "            container=False,\n",
        "            scale=8,\n",
        "            autofocus=True\n",
        "        )\n",
        "        clear = gr.Button(\"üóëÔ∏è Limpiar\", size=\"sm\", scale=1)\n",
        "\n",
        "    # Secci√≥n de instrucciones\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
        "            <h3 style=\"color: #2d333a; margin-bottom: 10px;\">üîç C√≥mo usar:</h3>\n",
        "            <ol style=\"color: #666; margin-left: 20px;\">\n",
        "                <li>Carga uno o m√°s documentos (PDF, DOCX, CSV, o TXT)</li>\n",
        "                <li>Espera a que los documentos sean procesados</li>\n",
        "                <li>Haz preguntas sobre el contenido de tus documentos</li>\n",
        "                <li>Activa \"Usar Ollama directo\" para respuestas m√°s r√°pidas (desact√≠valo si hay errores)</li>\n",
        "            </ol>\n",
        "            <p style=\"color: #666; font-style: italic; margin-top: 10px;\">\n",
        "                Nota: La primera respuesta puede tardar un poco. Desde la segunda respuesta es m√°s r√°pido.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Pie de p√°gina con informaci√≥n t√©cnica y cr√©ditos\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; max-width: 800px; margin: 20px auto; padding: 20px;\n",
        "                    background-color: #f8f9fa; border-radius: 10px;\">\n",
        "            <div style=\"margin-bottom: 15px;\">\n",
        "                <h3 style=\"color: #2d333a;\">‚ö° Sobre este asistente</h3>\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Esta aplicaci√≥n utiliza tecnolog√≠a RAG (Retrieval Augmented Generation) combinando:\n",
        "                </p>\n",
        "                <ul style=\"list-style: none; color: #666; font-size: 14px;\">\n",
        "                    <li>üîπ Motor LLM: Ollama con Llama2</li>\n",
        "                    <li>üîπ Embeddings: multilingual-e5-small</li>\n",
        "                    <li>üîπ Base de datos vectorial: FAISS</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            <div style=\"border-top: 1px solid #ddd; padding-top: 15px;\">\n",
        "                <p style=\"color: #666; font-size: 14px;\">\n",
        "                    Creado para el curso de Inteligencia Artificial Aplicada - Universidad de los Andes<br>\n",
        "                    Por <a href=\"https://www.linkedin.com/in/camilovegabarbosa/\"\n",
        "                    target=\"_blank\" style=\"color: #2196F3; text-decoration: none;\">Camilo Vega</a>,\n",
        "                    Profesor de IA ü§ñ\n",
        "                </p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # --------- FUNCIONES DE CONTROL DE LA INTERFAZ ---------\n",
        "    # Funci√≥n para limpiar el contexto y reiniciar\n",
        "    def clear_context():\n",
        "        # Eliminar la base de conocimiento y reiniciar el registro de archivos\n",
        "        rag_system.vector_store = None\n",
        "        rag_system.processed_files.clear()\n",
        "        return None\n",
        "\n",
        "    # Conectar eventos de la interfaz con funciones\n",
        "    message.submit(process_response, [message, chatbot, files, use_direct_ollama], [chatbot])\n",
        "    clear.click(clear_context, None, chatbot)\n",
        "\n",
        "# Lanzar la interfaz web\n",
        "print(\"üöÄ Lanzando interfaz Gradio...\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "5Yk--0McTuJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "85f33b6c-d1c6-4437-bc45-5af4f37ae0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Creando interfaz Gradio...\n",
            "üöÄ Lanzando interfaz Gradio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-43c02599d002>:42: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "<ipython-input-27-43c02599d002>:42: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://32964197637239b696.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://32964197637239b696.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}